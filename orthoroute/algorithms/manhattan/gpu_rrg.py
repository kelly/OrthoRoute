"""
GPU-Accelerated Routing Resource Graph (RRG)
Keeps the fabric intelligence, accelerates the data structures and algorithms
"""

import numpy as np
import logging
import time
from typing import Dict, List, Tuple, Optional, Set
from dataclasses import dataclass
from collections import defaultdict

from .rrg import (
    RoutingResourceGraph, RRGNode, RRGEdge, RoutingConfig, 
    RouteRequest, RouteResult, NodeType, EdgeType
)
from .gpu_pad_tap import GPUPadTapBuilder, PadTapConfig, TapCandidate

logger = logging.getLogger(__name__)

try:
    import cupy as cp
    import cupyx.scipy.sparse as cp_sparse
    GPU_AVAILABLE = True
except ImportError:
    import numpy as cp
    import scipy.sparse as cp_sparse
    GPU_AVAILABLE = False

@dataclass 
class GPUPathFindingState:
    """GPU arrays for PathFinder algorithm state"""
    # Routing costs (updated during negotiated congestion)
    node_cost: cp.ndarray           # [node_idx] -> current routing cost
    edge_cost: cp.ndarray           # [edge_idx] -> current routing cost
    
    # Congestion tracking
    node_usage: cp.ndarray          # [node_idx] -> current usage count
    edge_usage: cp.ndarray          # [edge_idx] -> current usage count
    node_capacity: cp.ndarray       # [node_idx] -> maximum capacity
    edge_capacity: cp.ndarray       # [edge_idx] -> maximum capacity
    
    # PathFinder congestion costs
    node_pres_cost: cp.ndarray      # [node_idx] -> present congestion cost
    edge_pres_cost: cp.ndarray      # [edge_idx] -> present congestion cost
    node_hist_cost: cp.ndarray      # [node_idx] -> historical congestion cost
    edge_hist_cost: cp.ndarray      # [edge_idx] -> historical congestion cost
    
    # Pathfinding workspace
    distance: cp.ndarray            # [node_idx] -> shortest distance
    parent_node: cp.ndarray         # [node_idx] -> parent node index
    parent_edge: cp.ndarray         # [node_idx] -> parent edge index
    visited: cp.ndarray             # [node_idx] -> visited flag

class GPURoutingResourceGraph:
    """GPU-accelerated RRG preserving fabric intelligence with PadTap support"""
    
    def __init__(self, cpu_rrg: RoutingResourceGraph, use_gpu: bool = True):
        """Initialize GPU RRG from existing CPU RRG"""
        self.config = cpu_rrg.config
        self.use_gpu = use_gpu and GPU_AVAILABLE
        
        logger.info(f"Converting RRG to GPU acceleration (GPU: {self.use_gpu})")
        start_time = time.time()
        
        # Store original CPU data for reference
        self.cpu_rrg = cpu_rrg
        
        # SAFETY CHECK: Estimate memory requirements before allocation
        estimated_nodes = len(cpu_rrg.nodes)
        estimated_edges = len(cpu_rrg.edges)
        
        # Calculate theoretical memory requirements
        if self.use_gpu:
            theoretical_memory_mb = self._estimate_theoretical_memory(estimated_nodes, estimated_edges)
            logger.info(f"Theoretical GPU memory requirement: {theoretical_memory_mb:.1f}MB")
            
            # EMERGENCY BRAKE: If memory requirement > 4GB, force CPU fallback
            if theoretical_memory_mb > 4000:
                logger.error(f"EMERGENCY: GPU RRG would require {theoretical_memory_mb:.1f}MB - forcing CPU fallback!")
                logger.error(f"Board has {estimated_nodes:,} nodes and {estimated_edges:,} edges")
                self.use_gpu = False
            # WARNING: If memory requirement > 1GB, warn but proceed
            elif theoretical_memory_mb > 1000:
                logger.warning(f"WARNING: GPU RRG requires {theoretical_memory_mb:.1f}MB - monitor for OOM!")
        
        # PadTap system for vertical pad escapes
        self.pad_tap_builder = None
        self.tap_candidates = {}  # net_name -> List[TapCandidate]
        self.tap_nodes = {}       # tap_id -> node_idx (in GPU arrays)
        self.tap_edges = {}       # edge_id -> edge_idx (in GPU arrays)
        
        try:
            # Convert nodes and edges to indexed arrays
            self._build_node_mappings()
            self._build_edge_mappings() 
            self._build_gpu_arrays()
            self._build_adjacency_matrix()
            
            # Initialize PathFinder state
            self.pathfinder_state = self._create_pathfinder_state()
            
            # Build spatial index for tap connections (CRITICAL for proper routing)
            self._build_spatial_index()
            
        except Exception as e:
            if "out of memory" in str(e).lower() or "cuda" in str(e).lower():
                logger.error(f"GPU OUT OF MEMORY during RRG construction: {e}")
                logger.error("Falling back to CPU mode")
                # Clean up any partial GPU allocations
                self._cleanup_gpu_memory()
                self.use_gpu = False
                # Retry with CPU mode
                self._build_node_mappings()
                self._build_edge_mappings() 
                self._build_gpu_arrays()
                self._build_adjacency_matrix()
                self.pathfinder_state = self._create_pathfinder_state()
                self._build_spatial_index()
            else:
                raise
        
        conversion_time = time.time() - start_time
        logger.info(f"GPU RRG conversion completed in {conversion_time:.2f}s")
        logger.info(f"Nodes: {self.num_nodes:,}, Edges: {self.num_edges:,}")
        
        if self.use_gpu:
            memory_usage = self._estimate_gpu_memory()
            logger.info(f"GPU memory usage: ~{memory_usage:.1f}MB")
    
    def _build_node_mappings(self):
        """Create bidirectional node mapping"""
        # Create node index mappings
        self.node_ids = list(self.cpu_rrg.nodes.keys())
        self.num_nodes = len(self.node_ids)
        self.node_id_to_idx = {node_id: idx for idx, node_id in enumerate(self.node_ids)}
        
        logger.debug(f"Created node mappings for {self.num_nodes} nodes")
    
    def _build_edge_mappings(self):
        """Create bidirectional edge mapping"""
        # Create edge index mappings  
        self.edge_ids = list(self.cpu_rrg.edges.keys())
        self.num_edges = len(self.edge_ids)
        self.edge_id_to_idx = {edge_id: idx for idx, edge_id in enumerate(self.edge_ids)}
        
        logger.debug(f"Created edge mappings for {self.num_edges} edges")
    
    def _build_gpu_arrays(self):
        """Convert RRG data to GPU arrays with memory monitoring"""
        
        # Monitor GPU memory before allocation
        if self.use_gpu:
            initial_memory = self._get_gpu_memory_usage()
            logger.info(f"GPU memory before array creation: {initial_memory:.1f}MB used")
        
        # Node data arrays
        node_data = self._extract_node_data()
        edge_data = self._extract_edge_data()
        
        if self.use_gpu:
            try:
                # Create GPU arrays with memory monitoring
                self.node_positions = cp.array(node_data['positions'], dtype=cp.float32)
                self.node_layers = cp.array(node_data['layers'], dtype=cp.int32)
                self.node_types = cp.array(node_data['types'], dtype=cp.int32)
                self.node_capacity = cp.array(node_data['capacity'], dtype=cp.int32)
                self.node_base_cost = cp.array(node_data['base_cost'], dtype=cp.float32)
                
                self.edge_lengths = cp.array(edge_data['lengths'], dtype=cp.float32)
                self.edge_types = cp.array(edge_data['types'], dtype=cp.int32) 
                self.edge_capacity = cp.array(edge_data['capacity'], dtype=cp.int32)
                self.edge_base_cost = cp.array(edge_data['base_cost'], dtype=cp.float32)
                self.edge_from_nodes = cp.array(edge_data['from_nodes'], dtype=cp.int32)
                self.edge_to_nodes = cp.array(edge_data['to_nodes'], dtype=cp.int32)
                
                # Monitor memory after allocation
                final_memory = self._get_gpu_memory_usage()
                array_memory = final_memory - initial_memory
                logger.info(f"GPU arrays created: {array_memory:.1f}MB allocated, {final_memory:.1f}MB total used")
                
                # Check for memory pressure
                if final_memory > 4000:  # 4GB threshold
                    logger.warning(f"HIGH GPU MEMORY USAGE: {final_memory:.1f}MB - risk of OOM!")
                elif final_memory > 2000:  # 2GB threshold
                    logger.warning(f"GPU memory usage elevated: {final_memory:.1f}MB")
                    
            except cp.cuda.memory.OutOfMemoryError as e:
                logger.error(f"GPU OUT OF MEMORY during array creation: {e}")
                logger.error("Falling back to CPU arrays")
                self.use_gpu = False
                # Fall through to CPU creation
            except Exception as e:
                logger.error(f"GPU array creation failed: {e}")
                logger.error("Falling back to CPU arrays")  
                self.use_gpu = False
                # Fall through to CPU creation
                
        if not self.use_gpu:
            # Create CPU arrays
            self.node_positions = np.array(node_data['positions'], dtype=np.float32)
            self.node_layers = np.array(node_data['layers'], dtype=np.int32)
            self.node_types = np.array(node_data['types'], dtype=np.int32)
            self.node_capacity = np.array(node_data['capacity'], dtype=np.int32)
            self.node_base_cost = np.array(node_data['base_cost'], dtype=np.float32)
            
            self.edge_lengths = np.array(edge_data['lengths'], dtype=np.float32)
            self.edge_types = np.array(edge_data['types'], dtype=np.int32)
            self.edge_capacity = np.array(edge_data['capacity'], dtype=np.int32)
            self.edge_base_cost = np.array(edge_data['base_cost'], dtype=np.float32)
            self.edge_from_nodes = np.array(edge_data['from_nodes'], dtype=np.int32)
            self.edge_to_nodes = np.array(edge_data['to_nodes'], dtype=np.int32)
            
            logger.info("Created CPU arrays for nodes and edges")
        
        # Skip spatial index for now - use direct connection approach
        logger.info("Using direct tap-to-fabric connection approach for performance")
    
    def _get_gpu_memory_usage(self) -> float:
        """Get current GPU memory usage in MB"""
        if not self.use_gpu:
            return 0.0
        try:
            mempool = cp.get_default_memory_pool()
            used_bytes = mempool.used_bytes()
            return used_bytes / (1024 * 1024)  # Convert to MB
        except:
            # Fallback using CUDA runtime API
            try:
                free, total = cp.cuda.runtime.memGetInfo()
                used = total - free
                return used / (1024 * 1024)  # Convert to MB
            except:
                return 0.0
                
    def cleanup_gpu_memory(self):
        """Clean up GPU memory to prevent leaks"""
        if not self.use_gpu:
            return
        try:
            # Force garbage collection
            import gc
            gc.collect()
            
            # Clear CuPy memory pool
            mempool = cp.get_default_memory_pool()
            pinned_mempool = cp.get_default_pinned_memory_pool()
            mempool.free_all_blocks()
            pinned_mempool.free_all_blocks()
            
            logger.info("GPU memory cleanup completed")
        except Exception as e:
            logger.warning(f"GPU memory cleanup failed: {e}")
    
    def _extract_node_data(self) -> Dict[str, List]:
        """Extract node data into arrays using memory-efficient numpy arrays"""
        
        # Pre-allocate numpy arrays instead of Python lists to save memory
        num_nodes = len(self.node_ids)
        positions = np.zeros((num_nodes, 2), dtype=np.float32)  # [x, y] positions
        layers = np.zeros(num_nodes, dtype=np.int32)
        types = np.zeros(num_nodes, dtype=np.int32)
        capacity = np.zeros(num_nodes, dtype=np.int32)
        base_cost = np.zeros(num_nodes, dtype=np.float32)
        
        # Create mapping from NodeType enum values to integers
        type_mapping = {
            "rail": 0,
            "bus": 1, 
            "switch": 2,
            "pad_entry": 3,
            "pad_exit": 4
        }
        
        logger.info(f"Extracting data for {num_nodes:,} nodes using memory-efficient arrays")
        
        for i, node_id in enumerate(self.node_ids):
            node = self.cpu_rrg.nodes[node_id]
            positions[i] = [node.x, node.y]
            layers[i] = node.layer
            
            # Convert node type to integer
            if hasattr(node.node_type, 'value'):
                type_val = type_mapping.get(node.node_type.value, 0)
            else:
                # Fallback if it's already a string
                type_val = type_mapping.get(str(node.node_type), 0)
            types[i] = type_val
            
            capacity[i] = node.capacity
            base_cost[i] = 1.0  # Base cost for routing
            
            # Progress logging for large datasets
            if i > 0 and i % 1000000 == 0:
                logger.info(f"Node data extraction progress: {i/num_nodes*100:.1f}% ({i:,}/{num_nodes:,})")
            
        return {
            'positions': positions,
            'layers': layers, 
            'types': types,
            'capacity': capacity,
            'base_cost': base_cost
        }
    
    def _extract_edge_data(self) -> Dict[str, List]:
        """Extract edge data into arrays using memory-efficient numpy arrays"""
        
        # Pre-allocate numpy arrays instead of Python lists to save memory
        num_edges = len(self.edge_ids)
        lengths = np.zeros(num_edges, dtype=np.float32)
        types = np.zeros(num_edges, dtype=np.int32)
        capacity = np.zeros(num_edges, dtype=np.int32)
        base_cost = np.zeros(num_edges, dtype=np.float32)
        from_nodes = np.zeros(num_edges, dtype=np.int32)
        to_nodes = np.zeros(num_edges, dtype=np.int32)
        
        # Create mapping from EdgeType enum values to integers
        edge_type_mapping = {
            "track": 0,
            "entry": 1,
            "exit": 2, 
            "switch": 3
        }
        
        logger.info(f"Extracting data for {num_edges:,} edges using memory-efficient arrays")
        
        for i, edge_id in enumerate(self.edge_ids):
            edge = self.cpu_rrg.edges[edge_id]
            lengths[i] = edge.length_mm
            
            # Convert edge type to integer
            if hasattr(edge.edge_type, 'value'):
                edge_type_val = edge_type_mapping.get(edge.edge_type.value, 0)
            else:
                # Fallback if it's already a string
                edge_type_val = edge_type_mapping.get(str(edge.edge_type), 0)
            types[i] = edge_type_val
            
            capacity[i] = edge.capacity
            
            # Calculate base cost from length and type
            cost = edge.length_mm * self.config.k_length
            if hasattr(edge, 'edge_type') and 'switch' in str(edge.edge_type):
                cost += self.config.k_via
            base_cost[i] = cost
            
            # Add from/to node indices
            from_nodes[i] = self.node_id_to_idx.get(edge.from_node, -1)
            to_nodes[i] = self.node_id_to_idx.get(edge.to_node, -1)
            
            # Progress logging for large datasets
            if i > 0 and i % 1000000 == 0:
                logger.info(f"Edge data extraction progress: {i/num_edges*100:.1f}% ({i:,}/{num_edges:,})")
        
        return {
            'lengths': lengths,
            'types': types,
            'capacity': capacity,
            'base_cost': base_cost,
            'from_nodes': from_nodes,
            'to_nodes': to_nodes
        }
    
    def _build_adjacency_matrix(self):
        """Build GPU-native sparse adjacency lists using efficient CSR format"""
        
        logger.info("Building sparse adjacency lists (CSR format)...")
        start_time = time.time()
        
        # Pre-calculate edge connectivity for memory efficiency
        edge_connections = []  # [(from_idx, to_idx, edge_idx, cost), ...]
        
        for edge_idx, edge_id in enumerate(self.edge_ids):
            if edge_id in self.cpu_rrg.edges:
                edge = self.cpu_rrg.edges[edge_id]
                from_idx = self.node_id_to_idx.get(edge.from_node, -1)
                to_idx = self.node_id_to_idx.get(edge.to_node, -1)
                
                if from_idx >= 0 and to_idx >= 0 and from_idx < self.num_nodes and to_idx < self.num_nodes:
                    base_cost = self.edge_base_cost[edge_idx] if hasattr(self, 'edge_base_cost') and edge_idx < len(self.edge_base_cost) else 1.0
                    # Bidirectional connections
                    edge_connections.append((from_idx, to_idx, edge_idx, base_cost))
                    edge_connections.append((to_idx, from_idx, edge_idx, base_cost))
        
        logger.info(f"Found {len(edge_connections)} directional connections from {len(self.edge_ids)} edges")
        
        if self.use_gpu:
            # Build GPU CSR adjacency structure
            self._build_gpu_csr_matrix(edge_connections)
        else:
            # Build CPU sparse matrix
            self._build_cpu_csr_matrix(edge_connections)
        
        construction_time = time.time() - start_time
        memory_mb = self._estimate_adjacency_memory_mb()
        logger.info(f"Sparse adjacency lists built in {construction_time:.2f}s")
        logger.info(f"Memory usage: {memory_mb:.1f}MB (vs {self._estimate_dense_matrix_memory_mb():.1f}MB for dense)")
        logger.info(f"Sparsity: {len(edge_connections)} connections for {self.num_nodes:,} nodes")
    
    def _build_gpu_csr_matrix(self, edge_connections):
        """Build GPU CSR matrix from edge connections"""
        if not edge_connections:
            # Empty graph - create minimal CSR structure
            self.adjacency_csr = cp_sparse.csr_matrix((self.num_nodes, self.num_nodes), dtype=cp.float32)
            self.neighbor_indices = cp.array([], dtype=cp.int32)
            self.neighbor_costs = cp.array([], dtype=cp.float32)
            self.node_offsets = cp.zeros(self.num_nodes + 1, dtype=cp.int32)
            return
        
        # Sort connections by source node for CSR format
        edge_connections.sort(key=lambda x: x[0])
        
        # Extract CSR data
        row_indices = []
        col_indices = []
        costs = []
        original_edge_indices = []  # Track original edge indices for PathFinder state
        
        for from_idx, to_idx, edge_idx, cost in edge_connections:
            row_indices.append(from_idx)
            col_indices.append(to_idx)
            costs.append(cost)
            original_edge_indices.append(edge_idx)
        
        # Convert to GPU arrays
        row_indices = cp.array(row_indices, dtype=cp.int32)
        col_indices = cp.array(col_indices, dtype=cp.int32)
        costs = cp.array(costs, dtype=cp.float32)
        self.csr_to_edge_mapping = cp.array(original_edge_indices, dtype=cp.int32)
        
        # Create CSR matrix
        self.adjacency_csr = cp_sparse.csr_matrix(
            (costs, (row_indices, col_indices)),
            shape=(self.num_nodes, self.num_nodes),
            dtype=cp.float32
        )
        
        # Store pathfinder-friendly format for fast neighbor lookup
        self.neighbor_indices = col_indices  # Which nodes each connection goes to
        self.neighbor_costs = costs         # Cost of each connection
        self.node_offsets = self.adjacency_csr.indptr  # Where each node's neighbors start
        
        # Keep old adjacency_matrix reference for backward compatibility
        self.adjacency_matrix = self.adjacency_csr
        
        logger.info(f"GPU CSR matrix: {self.adjacency_csr.nnz:,} non-zeros, shape {self.adjacency_csr.shape}")
    
    def _build_cpu_csr_matrix(self, edge_connections):
        """Build CPU sparse matrix from edge connections"""
        import scipy.sparse as scipy_sparse
        
        if not edge_connections:
            self.adjacency_matrix = scipy_sparse.csr_matrix((self.num_nodes, self.num_nodes), dtype=np.float32)
            return
        
        # Sort connections by source node
        edge_connections.sort(key=lambda x: x[0])
        
        row_indices = []
        col_indices = []
        costs = []
        original_edge_indices = []
        
        for from_idx, to_idx, edge_idx, cost in edge_connections:
            row_indices.append(from_idx)
            col_indices.append(to_idx)
            costs.append(cost)
            original_edge_indices.append(edge_idx)
        
        self.adjacency_matrix = scipy_sparse.csr_matrix(
            (costs, (row_indices, col_indices)),
            shape=(self.num_nodes, self.num_nodes),
            dtype=np.float32
        )
        self.csr_to_edge_mapping = np.array(original_edge_indices, dtype=np.int32)
        
        logger.info(f"CPU CSR matrix: {self.adjacency_matrix.nnz:,} non-zeros, shape {self.adjacency_matrix.shape}")
    
    def _estimate_adjacency_memory_mb(self):
        """Estimate memory usage of sparse adjacency structure"""
        if self.use_gpu and hasattr(self, 'adjacency_csr'):
            # CSR: data + indices + indptr
            nnz = self.adjacency_csr.nnz
            memory_bytes = nnz * 4 + nnz * 4 + (self.num_nodes + 1) * 4  # float32 + int32 + int32
        elif hasattr(self, 'adjacency_matrix') and hasattr(self.adjacency_matrix, 'nnz'):
            nnz = self.adjacency_matrix.nnz
            memory_bytes = nnz * 4 + nnz * 4 + (self.num_nodes + 1) * 4
        else:
            memory_bytes = 0
        return memory_bytes / (1024 * 1024)
    
    def _estimate_dense_matrix_memory_mb(self):
        """Calculate what dense matrix would cost"""
        return (self.num_nodes * self.num_nodes * 4) / (1024 * 1024)  # float32
    
    def _create_pathfinder_state(self) -> GPUPathFindingState:
        """Initialize PathFinder algorithm state"""
        
        if self.use_gpu:
            # Ensure all base arrays are CuPy arrays before copying
            if not isinstance(self.node_base_cost, cp.ndarray):
                logger.error(f"node_base_cost is {type(self.node_base_cost)}, converting for PathFinder")
                self.node_base_cost = cp.asarray(self.node_base_cost, dtype=cp.float32)
            if not isinstance(self.edge_base_cost, cp.ndarray):
                logger.error(f"edge_base_cost is {type(self.edge_base_cost)}, converting for PathFinder")
                self.edge_base_cost = cp.asarray(self.edge_base_cost, dtype=cp.float32)
            if not isinstance(self.node_capacity, cp.ndarray):
                logger.error(f"node_capacity is {type(self.node_capacity)}, converting for PathFinder")
                self.node_capacity = cp.asarray(self.node_capacity, dtype=cp.int32)
            if not isinstance(self.edge_capacity, cp.ndarray):
                logger.error(f"edge_capacity is {type(self.edge_capacity)}, converting for PathFinder")
                self.edge_capacity = cp.asarray(self.edge_capacity, dtype=cp.int32)
                
            return GPUPathFindingState(
                # Routing costs (start with base costs)
                node_cost=cp.copy(self.node_base_cost),
                edge_cost=cp.copy(self.edge_base_cost),
                
                # Congestion tracking (start empty)
                node_usage=cp.zeros(self.num_nodes, dtype=cp.int32),
                edge_usage=cp.zeros(self.num_edges, dtype=cp.int32),
                node_capacity=cp.copy(self.node_capacity),
                edge_capacity=cp.copy(self.edge_capacity),
                
                # PathFinder congestion costs (start at zero)
                node_pres_cost=cp.zeros(self.num_nodes, dtype=cp.float32),
                edge_pres_cost=cp.zeros(self.num_edges, dtype=cp.float32), 
                node_hist_cost=cp.zeros(self.num_nodes, dtype=cp.float32),
                edge_hist_cost=cp.zeros(self.num_edges, dtype=cp.float32),
                
                # Pathfinding workspace
                distance=cp.full(self.num_nodes, cp.inf, dtype=cp.float32),
                parent_node=cp.full(self.num_nodes, -1, dtype=cp.int32),
                parent_edge=cp.full(self.num_nodes, -1, dtype=cp.int32),
                visited=cp.zeros(self.num_nodes, dtype=cp.bool_)
            )
        else:
            return GPUPathFindingState(
                # CPU versions
                node_cost=np.copy(self.node_base_cost),
                edge_cost=np.copy(self.edge_base_cost),
                
                node_usage=np.zeros(self.num_nodes, dtype=np.int32),
                edge_usage=np.zeros(self.num_edges, dtype=np.int32),
                node_capacity=np.copy(self.node_capacity),
                edge_capacity=np.copy(self.edge_capacity),
                
                node_pres_cost=np.zeros(self.num_nodes, dtype=np.float32),
                edge_pres_cost=np.zeros(self.num_edges, dtype=np.float32),
                node_hist_cost=np.zeros(self.num_nodes, dtype=np.float32),
                edge_hist_cost=np.zeros(self.num_edges, dtype=np.float32),
                
                distance=np.full(self.num_nodes, np.inf, dtype=np.float32),
                parent_node=np.full(self.num_nodes, -1, dtype=np.int32),
                parent_edge=np.full(self.num_nodes, -1, dtype=np.int32),
                visited=np.zeros(self.num_nodes, dtype=np.bool_)
            )
    
    def get_node_idx(self, node_id: str) -> Optional[int]:
        """Get node index from node ID"""
        return self.node_id_to_idx.get(node_id)
    
    def get_edge_idx(self, edge_id: str) -> Optional[int]:
        """Get edge index from edge ID"""
        return self.edge_id_to_idx.get(edge_id)
    
    def get_node_id(self, node_idx: int) -> str:
        """Get node ID from node index"""
        return self.node_ids[node_idx] if 0 <= node_idx < self.num_nodes else ""
    
    def get_edge_id(self, edge_idx: int) -> str:
        """Get edge ID from edge index"""
        return self.edge_ids[edge_idx] if 0 <= edge_idx < self.num_edges else ""
    
    def get_memory_usage(self) -> float:
        """Get current GPU memory usage in MB (public interface)"""
        return self._estimate_gpu_memory()
    
    def _estimate_gpu_memory(self) -> float:
        """Estimate GPU memory usage in MB"""
        if not self.use_gpu:
            return 0.0
        
        # Node arrays
        node_memory = self.num_nodes * (2 * 4 + 4 + 4 + 4 + 4)  # positions + 4 int32 fields
        
        # Edge arrays  
        edge_memory = self.num_edges * (4 + 4 + 4 + 4)  # 4 float32/int32 fields
        
        # PathFinder state
        state_memory = (self.num_nodes + self.num_edges) * 4 * 8  # 8 arrays per node/edge
        
        # CRITICAL: Adjacency matrix memory calculation
        # For dense matrix this would be: num_nodes * num_nodes * 4 bytes = MASSIVE!
        # For sparse matrix: only non-zero entries count
        if hasattr(self, 'adjacency_matrix') and self.adjacency_matrix is not None:
            adj_memory = len(self.adjacency_matrix.data) * 4 * 3  # data + indices + indptr
            
            # WARNING: Check if adjacency matrix is accidentally dense
            if hasattr(self.adjacency_matrix, 'shape'):
                theoretical_dense_size = self.num_nodes * self.num_nodes * 4 / (1024 * 1024)  # MB
                actual_sparse_size = adj_memory / (1024 * 1024)  # MB
                
                if theoretical_dense_size > 1000:  # > 1GB
                    logger.warning(f"CRITICAL: Dense adjacency matrix would use {theoretical_dense_size:.1f}MB!")
                    logger.warning(f"Using sparse representation: {actual_sparse_size:.1f}MB")
                    
                    # EMERGENCY: If sparse is still huge, something is wrong
                    if actual_sparse_size > 500:  # > 500MB for adjacency alone
                        logger.error(f"MEMORY LEAK: Adjacency matrix using {actual_sparse_size:.1f}MB - too large!")
                        logger.error(f"Nodes: {self.num_nodes:,}, Edges: {self.num_edges:,}")
                        logger.error("Consider reducing board complexity or using CPU fallback")
        else:
            adj_memory = 0
            logger.warning("Adjacency matrix not yet built, memory estimate incomplete")
        
        total_bytes = node_memory + edge_memory + state_memory + adj_memory
        total_mb = total_bytes / (1024 * 1024)
        
        # SAFETY CHECK: If total memory > 2GB, warn user
        if total_mb > 2000:
            logger.error(f"CRITICAL: GPU RRG requires {total_mb:.1f}MB - risk of OOM!")
            logger.error("Consider enabling CPU fallback or reducing board complexity")
            
        return total_mb
    
    def __del__(self):
        """Destructor to clean up GPU memory"""
        try:
            self._cleanup_gpu_memory()
        except:
            pass  # Ignore cleanup errors during destruction
    
    def _estimate_theoretical_memory(self, num_nodes: int, num_edges: int) -> float:
        """Estimate theoretical memory requirements before allocation"""
        # Node arrays (assuming typical sparsity)
        node_memory = num_nodes * (2 * 4 + 4 + 4 + 4 + 4)  # positions + 4 int32 fields
        
        # Edge arrays
        edge_memory = num_edges * (4 + 4 + 4 + 4)  # 4 float32/int32 fields
        
        # PathFinder state arrays (8 arrays per node/edge)
        state_memory = (num_nodes + num_edges) * 4 * 8
        
        # Adjacency matrix - CRITICAL calculation
        # Assume sparse matrix with average connectivity (typical PCB has ~6 connections per node)
        avg_connections_per_node = 6
        estimated_nonzeros = min(num_nodes * avg_connections_per_node, num_nodes * num_nodes)
        adj_memory = estimated_nonzeros * 4 * 3  # data + indices + indptr
        
        total_bytes = node_memory + edge_memory + state_memory + adj_memory
        return total_bytes / (1024 * 1024)  # Convert to MB
    
    def _cleanup_gpu_memory(self):
        """Clean up GPU memory allocations"""
        if not self.use_gpu:
            return
            
        try:
            import cupy as cp
            # Force garbage collection
            if hasattr(self, 'pathfinder_state'):
                del self.pathfinder_state
            if hasattr(self, 'adjacency_matrix'):
                del self.adjacency_matrix
                
            # Free all GPU memory pool blocks
            mempool = cp.get_default_memory_pool()
            mempool.free_all_blocks()
            logger.info("GPU memory cleanup completed")
        except Exception as e:
            logger.warning(f"GPU memory cleanup failed: {e}")
    
    def configure_pad_taps(self, pad_tap_config: PadTapConfig):
        """Configure PadTap system for on-demand use (no bulk generation)"""
        
        logger.info("Configuring PadTap system for on-demand tap generation...")
        
        # Create PadTap builder with configuration
        self.pad_tap_builder = GPUPadTapBuilder(pad_tap_config)
        
        # Extract grid rail positions from RRG for later use
        self.grid_rails = self._extract_vertical_rails()
        
        # Initialize tap candidates storage (empty for on-demand use)
        self.tap_candidates = {}
        
        logger.info(f"PadTap system configured for on-demand generation with {len(self.grid_rails)} grid rails")

    def initialize_pad_taps(self, pads: List[Dict], pad_tap_config: Optional[PadTapConfig] = None):
        """Initialize PadTap system for vertical pad escapes"""
        
        if pad_tap_config is None:
            pad_tap_config = PadTapConfig()
        
        logger.info("Initializing PadTap system for vertical pad escapes...")
        
        # Create PadTap builder
        self.pad_tap_builder = GPUPadTapBuilder(pad_tap_config)
        
        # Extract grid rail positions from RRG
        grid_rails = self._extract_vertical_rails()
        
        # Generate tap candidates for all pads
        self.tap_candidates = self.pad_tap_builder.build_tap_candidates(
            pads=pads, 
            grid_x_positions=grid_rails,
            num_layers=12  # F.Cu + 11 internal layers
        )
        
        # Augment RRG with tap nodes and edges
        self._add_tap_nodes_to_rrg()
        self._add_tap_edges_to_rrg()
        
        logger.info(f"PadTap system initialized: {len(self.tap_candidates)} nets with tap candidates")
        
    def _extract_vertical_rails(self) -> List[float]:
        """Extract vertical rail X positions from existing RRG or sparse builder"""
        
        # First try to get grid positions directly from sparse builder
        if hasattr(self, 'sparse_builder') and hasattr(self.sparse_builder, 'grid_x_positions'):
            positions = self.sparse_builder.grid_x_positions
            if positions:
                logger.info(f"Using {len(positions)} grid positions from sparse builder")
                return positions
        
        # Fallback: extract from RRG nodes
        rail_positions = set()
        
        # Find all RAIL nodes (vertical tracks)
        for node_id, node in self.cpu_rrg.nodes.items():
            if node.node_type == NodeType.RAIL:
                rail_positions.add(node.x)
        
        positions = sorted(list(rail_positions))
        logger.info(f"Extracted {len(positions)} rail positions from RRG nodes")
        return positions
    
    def _add_tap_nodes_to_rrg(self):
        """Add tap nodes to RRG node arrays"""
        
        new_nodes = []
        tap_node_id = f"tap_{len(self.node_ids)}"
        
        logger.info(f"TAP STORAGE LOOP: Processing {len(self.tap_candidates)} nets with tap candidates")
        for net_name, tap_list in self.tap_candidates.items():
            logger.info(f"TAP STORAGE: Processing net {net_name} with {len(tap_list)} tap candidates")
            for tap_idx, tap in enumerate(tap_list):
                # Create unique tap node ID
                tap_id = f"tap_{net_name}_{tap_idx}"
                
                # Create RRG node for this tap
                tap_node = RRGNode(
                    id=tap_id,
                    node_type=NodeType.SWITCH,  # Tap acts as layer switch point
                    x=tap.tap_x,
                    y=tap.tap_y, 
                    layer=tap.via_layers[1],  # Target layer (In1, In2, etc.)
                    capacity=1  # Single connection per tap
                )
                
                # Add to CPU RRG for reference
                self.cpu_rrg.nodes[tap_id] = tap_node
                
                # Track tap node mapping
                new_node_idx = len(self.node_ids)
                self.node_ids.append(tap_id)
                self.node_id_to_idx[tap_id] = new_node_idx
                self.tap_nodes[tap_id] = new_node_idx
                logger.info(f"STORED TAP NODE: {tap_id} -> {new_node_idx} (total tap nodes: {len(self.tap_nodes)})")
                
                new_nodes.append(tap_node)
        
        if new_nodes:
            logger.info(f"Added {len(new_nodes)} tap nodes to RRG")
            
            # Rebuild GPU arrays to include new nodes
            self._rebuild_gpu_arrays_with_taps()
    
    def _add_tap_edges_to_rrg(self):
        """Add pad→tap and tap→grid edges to RRG"""
        
        new_edges = []
        
        for net_name, tap_list in self.tap_candidates.items():
            for tap_idx, tap in enumerate(tap_list):
                tap_id = f"tap_{net_name}_{tap_idx}"
                
                # Find corresponding pad entry node
                pad_entry_id = f"pad_entry_{net_name}_0"  # Assuming single source pad per net
                
                if pad_entry_id in self.cpu_rrg.nodes:
                    # 1. Pad → Tap edge (F.Cu escape trace)
                    escape_edge_id = f"escape_{pad_entry_id}_to_{tap_id}"
                    escape_edge = RRGEdge(
                        id=escape_edge_id,
                        from_node=pad_entry_id,
                        to_node=tap_id,
                        edge_type=EdgeType.ENTRY,
                        length_mm=tap.escape_length,
                        capacity=1
                    )
                    
                    self.cpu_rrg.edges[escape_edge_id] = escape_edge
                    new_edges.append(escape_edge)
                    
                    # 2. Tap → Grid edge (via connection) 
                    grid_node = self._find_nearest_grid_node(tap.tap_x, tap.tap_y, tap.via_layers[1])
                    if grid_node:
                        via_edge_id = f"via_{tap_id}_to_{grid_node}"
                        via_edge = RRGEdge(
                            id=via_edge_id,
                            from_node=tap_id,
                            to_node=grid_node,
                            edge_type=EdgeType.SWITCH,  # Via connection
                            length_mm=0.0,  # Via length negligible
                            capacity=1
                        )
                        
                        self.cpu_rrg.edges[via_edge_id] = via_edge
                        new_edges.append(via_edge)
        
        if new_edges:
            logger.info(f"Added {len(new_edges)} tap edges to RRG")
            
            # Update edge mappings
            for edge in new_edges:
                new_edge_idx = len(self.edge_ids)
                self.edge_ids.append(edge.id)
                self.edge_id_to_idx[edge.id] = new_edge_idx
                self.tap_edges[edge.id] = new_edge_idx
    
    def _find_nearest_grid_node(self, x: float, y: float, layer: int) -> Optional[str]:
        """Find nearest grid node (RAIL/BUS) at specified position and layer"""
        
        min_distance = float('inf')
        nearest_node = None
        
        # Search for grid nodes near tap position
        for node_id, node in self.cpu_rrg.nodes.items():
            if node.layer == layer and node.node_type in [NodeType.RAIL, NodeType.BUS]:
                distance = ((node.x - x) ** 2 + (node.y - y) ** 2) ** 0.5
                
                if distance < min_distance:
                    min_distance = distance
                    nearest_node = node_id
                    
                # If exact match, use it
                if distance < 0.1:  # 0.1mm tolerance
                    break
        
        return nearest_node
    
    def _rebuild_gpu_arrays_with_taps(self):
        """Rebuild GPU arrays to include new tap nodes"""
        
        # Re-extract all node data (including new tap nodes)
        node_data = self._extract_node_data()
        edge_data = self._extract_edge_data()
        
        # Update node and edge counts BEFORE creating PathFinder state
        self.num_nodes = len(self.node_ids)
        self.num_edges = len(self.edge_ids)
        
        # Update GPU arrays - ensure numpy arrays are properly converted
        if self.use_gpu:
            # Convert numpy arrays to CuPy arrays with explicit type conversion
            self.node_positions = cp.asarray(node_data['positions'], dtype=cp.float32)
            self.node_layers = cp.asarray(node_data['layers'], dtype=cp.int32)
            self.node_types = cp.asarray(node_data['types'], dtype=cp.int32)
            self.node_capacity = cp.asarray(node_data['capacity'], dtype=cp.int32)
            self.node_base_cost = cp.asarray(node_data['base_cost'], dtype=cp.float32)
            
            self.edge_lengths = cp.asarray(edge_data['lengths'], dtype=cp.float32)
            self.edge_types = cp.asarray(edge_data['types'], dtype=cp.int32)
            self.edge_capacity = cp.asarray(edge_data['capacity'], dtype=cp.int32)
            self.edge_base_cost = cp.asarray(edge_data['base_cost'], dtype=cp.float32)
            self.edge_from_nodes = cp.asarray(edge_data['from_nodes'], dtype=cp.int32)
            self.edge_to_nodes = cp.asarray(edge_data['to_nodes'], dtype=cp.int32)
        else:
            # CPU arrays
            self.node_positions = np.array(node_data['positions'], dtype=np.float32)
            self.node_layers = np.array(node_data['layers'], dtype=np.int32)
            self.node_types = np.array(node_data['types'], dtype=np.int32)
            self.node_capacity = np.array(node_data['capacity'], dtype=np.int32)
            self.node_base_cost = np.array(node_data['base_cost'], dtype=np.float32)
            
            self.edge_lengths = np.array(edge_data['lengths'], dtype=np.float32)
            self.edge_types = np.array(edge_data['types'], dtype=np.int32)
            self.edge_capacity = np.array(edge_data['capacity'], dtype=np.int32)
            self.edge_base_cost = np.array(edge_data['base_cost'], dtype=np.float32)
            self.edge_from_nodes = np.array(edge_data['from_nodes'], dtype=np.int32)
            self.edge_to_nodes = np.array(edge_data['to_nodes'], dtype=np.int32)
        
        # Rebuild adjacency matrix with new connections
        self._build_adjacency_matrix()
        
        # Update PathFinder state arrays with correct sizes
        self.pathfinder_state = self._create_pathfinder_state()
        
        logger.info(f"GPU arrays rebuilt with tap nodes and edges (nodes: {self.num_nodes}, edges: {self.num_edges})")
    
    def cleanup(self):
        """Clean up GPU memory"""
        if self.use_gpu:
            try:
                # Clean up main arrays
                del self.node_positions, self.node_layers, self.node_types
                del self.node_capacity, self.node_base_cost
                del self.edge_lengths, self.edge_types, self.edge_capacity, self.edge_base_cost
                del self.adjacency_matrix
                
                # Clean up PathFinder state
                state = self.pathfinder_state
                del state.node_cost, state.edge_cost
                del state.node_usage, state.edge_usage
                del state.node_capacity, state.edge_capacity
                del state.node_pres_cost, state.edge_pres_cost
                del state.node_hist_cost, state.edge_hist_cost
                del state.distance, state.parent_node, state.parent_edge, state.visited
                
                # Clean up PadTap builder
                if self.pad_tap_builder:
                    self.pad_tap_builder.cleanup()
                
                cp.get_default_memory_pool().free_all_blocks()
                logger.info("GPU RRG memory cleaned up")
            except:
                pass
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()
    
    def add_temporary_taps_for_net(self, net_name: str, net_pads: List[Dict]) -> Dict[str, List]:
        """ATOMIC tap node integration with comprehensive validation and rollback"""
        logger.info(f"ATOMIC TAP INTEGRATION: Starting for net {net_name}")
        
        # MEMORY SAFETY: Check current memory usage before adding taps
        current_memory = self._estimate_gpu_memory()
        if current_memory > 2000:  # > 2GB
            logger.error(f"MEMORY LIMIT: Current usage {current_memory:.1f}MB - refusing to add taps")
            return {'error': ['Memory limit exceeded']}
            
        # Estimate additional memory required for taps
        estimated_tap_memory = len(net_pads) * 100  # Rough estimate: 100MB per pad tap
        if current_memory + estimated_tap_memory > 3000:  # > 3GB total
            logger.warning(f"MEMORY WARNING: Adding taps would use {current_memory + estimated_tap_memory:.1f}MB total")
            logger.warning("Consider reducing tap complexity or using CPU fallback")
        
        # Validate prerequisites
        if not hasattr(self, 'pad_tap_builder') or self.pad_tap_builder is None:
            logger.error("PadTap builder not configured - call configure_pad_taps() first")
            return {}
            
        if not hasattr(self, 'grid_rails') or self.grid_rails is None:
            logger.error("Grid rails not available - PadTap system not properly configured")
            return {}
        
        # STEP 1: Capture complete state snapshot for rollback
        original_state = self._capture_atomic_state()
        
        try:
            # STEP 2: Generate tap candidates
            tap_candidates_list = self.pad_tap_builder.build_tap_candidates_for_net(
                net_pads=net_pads,
                grid_x_positions=self.grid_rails,
                num_layers=12
            )
            
            if not tap_candidates_list:
                logger.warning(f"No tap candidates generated for net {net_name}")
                return {}
            
            # STEP 3: Pre-validate tap-to-grid connections BEFORE modifying state
            logger.info(f"CRITICAL: Pre-validating {len(tap_candidates_list)} tap connections...")
            valid_connections = self._prevalidate_tap_connections(tap_candidates_list, net_name)
            
            if not valid_connections:
                logger.error(f"CRITICAL: No valid tap-to-grid connections found for {net_name}")
                return {}
            
            logger.info(f"PRE-VALIDATION SUCCESS: {len(valid_connections)} valid connections for {net_name}")
            
            # CRITICAL FIX: Store tap candidates for this net so _add_tap_nodes_to_rrg() can find them
            self.tap_candidates[net_name] = tap_candidates_list
            logger.info(f"STORED TAP CANDIDATES: {net_name} -> {len(tap_candidates_list)} candidates")
            
            # STEP 4: Calculate new dimensions
            new_node_count = len(tap_candidates_list)
            new_num_nodes = self.num_nodes + new_node_count
            new_num_edges = self.num_edges + len(valid_connections)
            
            logger.info(f"ATOMIC UPDATE: nodes {self.num_nodes} -> {new_num_nodes}, "
                       f"edges {self.num_edges} -> {new_num_edges}")
            
            # STEP 5: Atomic data structure updates
            self._atomic_add_tap_nodes(tap_candidates_list, net_name, new_num_nodes)
            self._atomic_add_tap_edges(valid_connections, new_num_edges)
            self._atomic_rebuild_adjacency_matrix(new_num_nodes, valid_connections)
            self._atomic_extend_pathfinder_state(new_num_nodes)
            
            # STEP 6: Comprehensive validation of final state
            logger.info(f"VALIDATION: Verifying complete system consistency...")
            validation_results = self._validate_atomic_integration()
            
            if not validation_results['overall_valid']:
                # Log detailed validation failures
                logger.error(f"ATOMIC INTEGRATION FAILED - VALIDATION ERRORS:")
                for category, result in validation_results.items():
                    if not result.get('valid', True):
                        logger.error(f"  {category}: {result.get('issues', [])}")
                        
                raise RuntimeError(f"Post-integration validation failed: {validation_results}")
            
            logger.info(f"ATOMIC TAP INTEGRATION SUCCESS: {net_name} with {new_node_count} nodes")
            return {net_name: tap_candidates_list}
            
        except Exception as e:
            # STEP 7: Atomic rollback on ANY failure
            logger.error(f"ATOMIC TAP INTEGRATION FAILED for {net_name}: {e}")
            logger.error(f"Rolling back to original state...")
            
            try:
                self._atomic_rollback(original_state)
                logger.info(f"Rollback completed - system restored to consistent state")
            except Exception as rollback_error:
                logger.error(f"CRITICAL: Rollback failed: {rollback_error}")
                raise RuntimeError(f"Both integration and rollback failed: {e}, {rollback_error}")
            
            raise RuntimeError(f"Tap integration failed and rolled back: {e}")
    
    def _capture_atomic_state(self) -> Dict:
        """Capture complete state snapshot for atomic rollback"""
        return {
            'num_nodes': self.num_nodes,
            'num_edges': self.num_edges,
            'node_ids': self.node_ids.copy(),
            'node_id_to_idx': self.node_id_to_idx.copy(),
            'edge_ids': self.edge_ids.copy(),
            'edge_id_to_idx': self.edge_id_to_idx.copy(),
            # Note: We don't copy large arrays - instead we store their sizes for truncation
            'node_array_sizes': {
                'positions': self.node_positions.shape[0] if hasattr(self, 'node_positions') else 0,
                'layers': self.node_layers.shape[0] if hasattr(self, 'node_layers') else 0,
                'types': self.node_types.shape[0] if hasattr(self, 'node_types') else 0,
                'capacity': self.node_capacity.shape[0] if hasattr(self, 'node_capacity') else 0,
                'base_cost': self.node_base_cost.shape[0] if hasattr(self, 'node_base_cost') else 0,
            },
            'edge_array_sizes': {
                'lengths': self.edge_lengths.shape[0] if hasattr(self, 'edge_lengths') else 0,
                'types': self.edge_types.shape[0] if hasattr(self, 'edge_types') else 0,
                'capacity': self.edge_capacity.shape[0] if hasattr(self, 'edge_capacity') else 0,
                'base_cost': self.edge_base_cost.shape[0] if hasattr(self, 'edge_base_cost') else 0,
                'from_nodes': self.edge_from_nodes.shape[0] if hasattr(self, 'edge_from_nodes') else 0,
                'to_nodes': self.edge_to_nodes.shape[0] if hasattr(self, 'edge_to_nodes') else 0,
            }
        }
    
    def _prevalidate_tap_connections(self, tap_candidates_list: List, net_name: str) -> List[Dict]:
        """Pre-validate ALL tap connections before modifying any state"""
        valid_connections = []
        
        for i, tap in enumerate(tap_candidates_list):
            # Multiple connection strategies for robustness
            connections = self._find_robust_grid_connections(tap.tap_x, tap.tap_y, tap.via_layers[1])
            
            if connections:
                valid_connections.extend(connections)
                logger.debug(f"Tap {i}: Found {len(connections)} grid connections")
            else:
                logger.error(f"CRITICAL: Tap {i} at ({tap.tap_x}, {tap.tap_y}) has NO grid connections")
                return []  # Fail fast - if ANY tap can't connect, abort entire operation
        
        logger.info(f"Pre-validation SUCCESS: All {len(tap_candidates_list)} taps have valid connections")
        return valid_connections
    
    def _find_robust_grid_connections(self, tap_x: float, tap_y: float, layer: int) -> List[Dict]:
        """Robust grid connection search with multiple fallback strategies"""
        
        logger.debug(f"Searching for grid connections near ({tap_x}, {tap_y}) on layer {layer}")
        
        # Strategy 1: Direct position lookup with tight tolerance
        nearby_nodes = self._search_grid_nodes_by_position(tap_x, tap_y, layer, tolerance=0.05)
        if nearby_nodes:
            logger.debug(f"Strategy 1 SUCCESS: Found {len(nearby_nodes)} nodes with 0.05mm tolerance")
            return self._create_connection_dicts(nearby_nodes[:2], tap_x, tap_y)
        
        # Strategy 2: Expanded radius search  
        nearby_nodes = self._search_grid_nodes_by_position(tap_x, tap_y, layer, tolerance=0.5)
        if nearby_nodes:
            logger.debug(f"Strategy 2 SUCCESS: Found {len(nearby_nodes)} nodes with 0.5mm tolerance")
            return self._create_connection_dicts(nearby_nodes[:2], tap_x, tap_y)
        
        # Strategy 3: Large radius search (for sparse test grids)
        nearby_nodes = self._search_grid_nodes_by_position(tap_x, tap_y, layer, tolerance=2.0)
        if nearby_nodes:
            logger.debug(f"Strategy 3 SUCCESS: Found {len(nearby_nodes)} nodes with 2.0mm tolerance")
            return self._create_connection_dicts(nearby_nodes[:2], tap_x, tap_y)
        
        # Strategy 4: Grid snapping (snap to 1.0mm grid for test)
        snapped_x = round(tap_x / 1.0) * 1.0
        snapped_y = round(tap_y / 1.0) * 1.0  
        nearby_nodes = self._search_grid_nodes_by_position(snapped_x, snapped_y, layer, tolerance=0.5)
        if nearby_nodes:
            logger.debug(f"Strategy 4 SUCCESS: Found {len(nearby_nodes)} nodes with grid snapping")
            return self._create_connection_dicts(nearby_nodes[:2], tap_x, tap_y)
        
        # Strategy 5: Adjacent layer search with larger tolerance
        for adj_layer in [layer-1, layer+1]:
            if adj_layer >= 0:
                nearby_nodes = self._search_grid_nodes_by_position(tap_x, tap_y, adj_layer, tolerance=2.0)
                if nearby_nodes:
                    logger.debug(f"Strategy 5 SUCCESS: Found {len(nearby_nodes)} nodes on layer {adj_layer}")
                    return self._create_connection_dicts(nearby_nodes[:1], tap_x, tap_y)
        
        # Strategy 6: Any node within very large radius (for sparse test cases)
        nearby_nodes = self._search_grid_nodes_by_position(tap_x, tap_y, -1, tolerance=5.0)  # -1 means any layer
        if nearby_nodes:
            logger.debug(f"Strategy 6 SUCCESS: Found {len(nearby_nodes)} nodes within 5.0mm on any layer")
            return self._create_connection_dicts(nearby_nodes[:1], tap_x, tap_y)
        
        logger.warning(f"All connection strategies FAILED for tap at ({tap_x}, {tap_y}) layer {layer}")
        return []  # All strategies failed
    
    def _search_grid_nodes_by_position(self, x: float, y: float, layer: int, tolerance: float) -> List[int]:
        """Search for grid nodes within tolerance of position"""
        nearby_nodes = []
        
        # Direct array search (optimized for performance)
        if hasattr(self, 'node_positions') and hasattr(self, 'node_layers'):
            
            # Special case: layer = -1 means search all layers
            if layer == -1:
                logger.debug(f"Searching ALL layers within tolerance {tolerance}")
                
                if self.use_gpu:
                    all_positions = self.node_positions
                    distances = cp.sqrt(cp.sum((all_positions - cp.array([x, y]))**2, axis=1))
                    close_mask = distances <= tolerance
                    close_indices = cp.where(close_mask)[0]
                    nearby_nodes = close_indices.tolist()[:10]
                else:
                    all_positions = self.node_positions
                    distances = np.sqrt(np.sum((all_positions - np.array([x, y]))**2, axis=1))
                    close_mask = distances <= tolerance
                    close_indices = np.where(close_mask)[0]
                    nearby_nodes = close_indices.tolist()[:10]
                    
                if nearby_nodes:
                    logger.debug(f"Found {len(nearby_nodes)} nodes on various layers within tolerance")
                    
            else:
                # First try exact layer match
                layer_mask = (self.node_layers == layer)
                if self.use_gpu:
                    layer_indices = cp.where(layer_mask)[0]
                    if len(layer_indices) > 0:
                        layer_positions = self.node_positions[layer_indices]
                        distances = cp.sqrt(cp.sum((layer_positions - cp.array([x, y]))**2, axis=1))
                        close_mask = distances <= tolerance
                        close_indices = layer_indices[close_mask]
                        nearby_nodes = close_indices.tolist()[:10]  # Limit results
                else:
                    layer_indices = np.where(layer_mask)[0]
                    if len(layer_indices) > 0:
                        layer_positions = self.node_positions[layer_indices]
                        distances = np.sqrt(np.sum((layer_positions - np.array([x, y]))**2, axis=1))
                        close_mask = distances <= tolerance
                        close_indices = layer_indices[close_mask]
                        nearby_nodes = close_indices.tolist()[:10]  # Limit results
                
                # If no nodes found on exact layer, try any layer within tolerance
                if not nearby_nodes:
                    logger.debug(f"No nodes found on layer {layer}, searching all layers within tolerance {tolerance}")
                    
                    if self.use_gpu:
                        all_positions = self.node_positions
                        distances = cp.sqrt(cp.sum((all_positions - cp.array([x, y]))**2, axis=1))
                        close_mask = distances <= tolerance
                        close_indices = cp.where(close_mask)[0]
                        nearby_nodes = close_indices.tolist()[:10]
                    else:
                        all_positions = self.node_positions
                        distances = np.sqrt(np.sum((all_positions - np.array([x, y]))**2, axis=1))
                        close_mask = distances <= tolerance
                        close_indices = np.where(close_mask)[0]
                        nearby_nodes = close_indices.tolist()[:10]
                    
                    if nearby_nodes:
                        logger.debug(f"Found {len(nearby_nodes)} nodes on various layers within tolerance")
        
        return nearby_nodes
    
    def _create_connection_dicts(self, node_indices: List[int], tap_x: float, tap_y: float) -> List[Dict]:
        """Create connection dictionaries for valid grid nodes"""
        connections = []
        
        for node_idx in node_indices:
            if 0 <= node_idx < self.num_nodes:  # Bounds check
                connections.append({
                    'from_node': self.num_nodes,  # Tap node will be at this index
                    'to_node': node_idx,         # Existing grid node
                    'edge_idx': self.num_edges,  # New edge index
                    'length': 0.2,               # Short connection
                    'type': 3,                   # Tap connection type
                    'cost': 1.0
                })
        
        return connections
    
    def _atomic_rebuild_adjacency_matrix(self, new_num_nodes: int, tap_connections: List[Dict]):
        """GPU-accelerated adjacency matrix rebuilding for tap node integration"""
        
        logger.info(f"GPU ADJACENCY REBUILD: Building matrix for {new_num_nodes} nodes with {len(tap_connections)} new tap connections")
        
        try:
            if self.use_gpu:
                import cupy as cp
                import cupyx.scipy.sparse as cp_sparse
                
                # STEP 1: Extract existing connections efficiently using GPU operations
                logger.info("Extracting existing connections...")
                old_matrix = self.adjacency_matrix
                coo_matrix = old_matrix.tocoo()
                
                # Extract connection data (GPU arrays)
                existing_rows = coo_matrix.row
                existing_cols = coo_matrix.col
                existing_data = coo_matrix.data
                
                num_existing = len(existing_data)
                num_new_connections = len(tap_connections) * 2  # bidirectional
                total_connections = num_existing + num_new_connections
                
                logger.info(f"GPU MATRIX: {num_existing} existing + {num_new_connections} new = {total_connections} connections")
                
                # STEP 2: Build complete connection arrays on GPU
                # Allocate GPU arrays for all connections
                all_rows = cp.zeros(total_connections, dtype=cp.int32)
                all_cols = cp.zeros(total_connections, dtype=cp.int32)
                all_data = cp.zeros(total_connections, dtype=cp.float32)
                
                # Copy existing connections
                all_rows[:num_existing] = existing_rows
                all_cols[:num_existing] = existing_cols
                all_data[:num_existing] = existing_data
                
                # STEP 3: Add bidirectional tap connections using vectorized operations
                if num_new_connections > 0:
                    # Extract tap connection data into arrays
                    tap_from_nodes = cp.array([conn['from_node'] for conn in tap_connections], dtype=cp.int32)
                    tap_to_nodes = cp.array([conn['to_node'] for conn in tap_connections], dtype=cp.int32)
                    tap_edge_indices = cp.array([float(conn['edge_idx']) for conn in tap_connections], dtype=cp.float32)
                    
                    # Add forward connections (tap -> grid)
                    start_idx = num_existing
                    end_idx = start_idx + len(tap_connections)
                    all_rows[start_idx:end_idx] = tap_from_nodes
                    all_cols[start_idx:end_idx] = tap_to_nodes
                    all_data[start_idx:end_idx] = tap_edge_indices
                    
                    # Add reverse connections (grid -> tap)  
                    start_idx = end_idx
                    end_idx = start_idx + len(tap_connections)
                    all_rows[start_idx:end_idx] = tap_to_nodes
                    all_cols[start_idx:end_idx] = tap_from_nodes
                    all_data[start_idx:end_idx] = tap_edge_indices
                
                # STEP 4: Build new CSR matrix using GPU sparse operations
                logger.info("Building GPU CSR matrix...")
                new_matrix = cp_sparse.coo_matrix(
                    (all_data, (all_rows, all_cols)),
                    shape=(new_num_nodes, new_num_nodes),
                    dtype=cp.float32
                ).tocsr()
                
                # STEP 5: Validate matrix properties
                assert new_matrix.shape == (new_num_nodes, new_num_nodes), f"Matrix shape mismatch: {new_matrix.shape} != ({new_num_nodes}, {new_num_nodes})"
                assert len(new_matrix.indptr) == new_num_nodes + 1, f"indptr size mismatch: {len(new_matrix.indptr)} != {new_num_nodes + 1}"
                
                self.adjacency_matrix = new_matrix
                
                logger.info(f"GPU ADJACENCY REBUILD SUCCESS: {new_matrix.shape} matrix with {len(new_matrix.data)} connections")
                
            else:
                # CPU fallback using same optimized approach
                import numpy as np
                import scipy.sparse as sp_sparse
                
                logger.info("Building CPU adjacency matrix...")
                old_matrix = self.adjacency_matrix
                coo_matrix = old_matrix.tocoo()
                
                existing_rows = coo_matrix.row
                existing_cols = coo_matrix.col
                existing_data = coo_matrix.data
                
                num_existing = len(existing_data)
                num_new_connections = len(tap_connections) * 2
                total_connections = num_existing + num_new_connections
                
                # Build complete connection arrays
                all_rows = np.zeros(total_connections, dtype=np.int32)
                all_cols = np.zeros(total_connections, dtype=np.int32)
                all_data = np.zeros(total_connections, dtype=np.float32)
                
                # Copy existing
                all_rows[:num_existing] = existing_rows
                all_cols[:num_existing] = existing_cols
                all_data[:num_existing] = existing_data
                
                # Add tap connections
                if num_new_connections > 0:
                    tap_from_nodes = np.array([conn['from_node'] for conn in tap_connections], dtype=np.int32)
                    tap_to_nodes = np.array([conn['to_node'] for conn in tap_connections], dtype=np.int32)
                    tap_edge_indices = np.array([float(conn['edge_idx']) for conn in tap_connections], dtype=np.float32)
                    
                    start_idx = num_existing
                    end_idx = start_idx + len(tap_connections)
                    all_rows[start_idx:end_idx] = tap_from_nodes
                    all_cols[start_idx:end_idx] = tap_to_nodes
                    all_data[start_idx:end_idx] = tap_edge_indices
                    
                    start_idx = end_idx
                    end_idx = start_idx + len(tap_connections)
                    all_rows[start_idx:end_idx] = tap_to_nodes
                    all_cols[start_idx:end_idx] = tap_from_nodes
                    all_data[start_idx:end_idx] = tap_edge_indices
                
                # Build matrix
                new_matrix = sp_sparse.coo_matrix(
                    (all_data, (all_rows, all_cols)),
                    shape=(new_num_nodes, new_num_nodes),
                    dtype=np.float32
                ).tocsr()
                
                self.adjacency_matrix = new_matrix
                logger.info(f"CPU ADJACENCY REBUILD SUCCESS: {new_matrix.shape} matrix with {len(new_matrix.data)} connections")
                
        except Exception as e:
            logger.error(f"ADJACENCY REBUILD FAILED: {e}")
            logger.error(f"Falling back to original matrix size - this may cause routing failures")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            raise RuntimeError(f"Critical: Adjacency matrix rebuild failed: {e}")
    
    def _validate_atomic_integration(self) -> Dict[str, Dict]:
        """Comprehensive validation after atomic integration"""
        results = {}
        
        # Node consistency
        results['node_consistency'] = self._validate_node_arrays_consistency()
        
        # Adjacency matrix consistency  
        results['adjacency_consistency'] = self._validate_adjacency_matrix_consistency()
        
        # PathFinder state consistency
        results['pathfinder_consistency'] = self._validate_pathfinder_arrays_consistency()
        
        # Overall validation - handle mixed result types
        overall_valid = True
        for category, result in results.items():
            if isinstance(result, dict):
                if not result.get('valid', False):
                    overall_valid = False
            elif isinstance(result, bool):
                if not result:
                    overall_valid = False
            else:
                overall_valid = False
                
        results['overall_valid'] = overall_valid
        
        return results
    
    def _validate_adjacency_matrix_consistency(self) -> Dict:
        """Validate adjacency matrix - relaxed for edge-based routing"""
        issues = []
        
        # RELAXED VALIDATION: Matrix size can be smaller than num_nodes
        # PathFinder uses edge arrays for tap connectivity
        actual_shape = self.adjacency_matrix.shape
        
        if actual_shape[0] != actual_shape[1]:
            issues.append(f"Matrix not square: {actual_shape}")
            
        if len(self.adjacency_matrix.indptr) != actual_shape[0] + 1:
            issues.append(f"indptr size mismatch for matrix shape {actual_shape}")
        
        # Matrix can be smaller than num_nodes - this is expected with edge-based tap routing
        if actual_shape[0] < self.num_nodes:
            logger.info(f"EDGE-BASED ROUTING: Matrix {actual_shape} < {self.num_nodes} nodes (tap nodes in edge arrays)")
        
        return {'valid': len(issues) == 0, 'issues': issues}
    
    def _atomic_rollback(self, original_state: Dict):
        """Atomic rollback to original consistent state"""
        logger.info(f"ATOMIC ROLLBACK: Restoring original state...")
        
        # Restore counts
        self.num_nodes = original_state['num_nodes']
        self.num_edges = original_state['num_edges']
        
        # Restore ID mappings
        self.node_ids = original_state['node_ids']
        self.node_id_to_idx = original_state['node_id_to_idx']
        self.edge_ids = original_state['edge_ids'] 
        self.edge_id_to_idx = original_state['edge_id_to_idx']
        
        # Truncate arrays back to original sizes
        node_sizes = original_state['node_array_sizes']
        edge_sizes = original_state['edge_array_sizes']
        
        if hasattr(self, 'node_positions'):
            self.node_positions = self.node_positions[:node_sizes['positions']]
        if hasattr(self, 'node_layers'):
            self.node_layers = self.node_layers[:node_sizes['layers']]
        if hasattr(self, 'node_types'):
            self.node_types = self.node_types[:node_sizes['types']]
        if hasattr(self, 'node_capacity'):
            self.node_capacity = self.node_capacity[:node_sizes['capacity']]
        if hasattr(self, 'node_base_cost'):
            self.node_base_cost = self.node_base_cost[:node_sizes['base_cost']]
            
        if hasattr(self, 'edge_lengths'):
            self.edge_lengths = self.edge_lengths[:edge_sizes['lengths']]
        if hasattr(self, 'edge_types'):
            self.edge_types = self.edge_types[:edge_sizes['types']]
        if hasattr(self, 'edge_capacity'):
            self.edge_capacity = self.edge_capacity[:edge_sizes['capacity']]
        if hasattr(self, 'edge_base_cost'):
            self.edge_base_cost = self.edge_base_cost[:edge_sizes['base_cost']]
        if hasattr(self, 'edge_from_nodes'):
            self.edge_from_nodes = self.edge_from_nodes[:edge_sizes['from_nodes']]
        if hasattr(self, 'edge_to_nodes'):
            self.edge_to_nodes = self.edge_to_nodes[:edge_sizes['to_nodes']]
        
        # Rebuild adjacency matrix for original dimensions
        self._build_adjacency_matrix()
        
        # Restore PathFinder state
        if hasattr(self, 'pathfinder_state'):
            self.pathfinder_state = self._create_pathfinder_state()
        
        logger.info(f"ATOMIC ROLLBACK COMPLETE: Restored to {self.num_nodes} nodes, {self.num_edges} edges")
    
    def _atomic_add_tap_nodes(self, tap_candidates_list: List, net_name: str, new_num_nodes: int):
        """Atomically add tap nodes to all node arrays"""
        new_node_count = len(tap_candidates_list)
        logger.info(f"ATOMIC NODE ADD: Adding {new_node_count} nodes for {net_name}")
        
        # Extend node arrays
        if self.use_gpu:
            # GPU version - create new arrays and concatenate
            new_positions = cp.zeros((new_node_count, 2), dtype=cp.float32)
            new_layers = cp.zeros(new_node_count, dtype=cp.int32)
            new_types = cp.full(new_node_count, 2, dtype=cp.int32)  # switch type
            new_capacity = cp.ones(new_node_count, dtype=cp.int32)
            new_cost = cp.ones(new_node_count, dtype=cp.float32)
            
            for i, tap in enumerate(tap_candidates_list):
                new_positions[i, 0] = tap.tap_x
                new_positions[i, 1] = tap.tap_y
                new_layers[i] = tap.via_layers[1]
            
            # Concatenate to existing arrays
            self.node_positions = cp.vstack([self.node_positions, new_positions])
            self.node_layers = cp.concatenate([self.node_layers, new_layers])
            self.node_types = cp.concatenate([self.node_types, new_types])
            self.node_capacity = cp.concatenate([self.node_capacity, new_capacity])
            self.node_base_cost = cp.concatenate([self.node_base_cost, new_cost])
        else:
            # CPU version
            new_positions = np.zeros((new_node_count, 2), dtype=np.float32)
            new_layers = np.zeros(new_node_count, dtype=np.int32)
            new_types = np.full(new_node_count, 2, dtype=np.int32)
            new_capacity = np.ones(new_node_count, dtype=np.int32)
            new_cost = np.ones(new_node_count, dtype=np.float32)
            
            for i, tap in enumerate(tap_candidates_list):
                new_positions[i, 0] = tap.tap_x
                new_positions[i, 1] = tap.tap_y
                new_layers[i] = tap.via_layers[1]
            
            self.node_positions = np.vstack([self.node_positions, new_positions])
            self.node_layers = np.concatenate([self.node_layers, new_layers])
            self.node_types = np.concatenate([self.node_types, new_types])
            self.node_capacity = np.concatenate([self.node_capacity, new_capacity])
            self.node_base_cost = np.concatenate([self.node_base_cost, new_cost])
        
        # Update node mappings AFTER updating count to ensure consistency
        # CRITICAL FIX: Update num_nodes FIRST, then assign indices within valid range
        start_idx = self.num_nodes  # Store original count
        self.num_nodes = new_num_nodes  # Update count BEFORE assigning indices
        
        for i, tap in enumerate(tap_candidates_list):
            tap_id = f"tap_{net_name}_{i}"
            node_idx = start_idx + i  # This ensures indices are in range [start_idx, start_idx + count - 1]
            
            # BOUNDARY CHECK: Ensure index is within valid range
            if node_idx >= self.num_nodes:
                raise RuntimeError(f"CRITICAL: Tap node index {node_idx} >= num_nodes {self.num_nodes}")
            
            self.node_ids.append(tap_id)
            self.node_id_to_idx[tap_id] = node_idx
            # CRITICAL FIX: Store in tap_nodes for PathFinder lookups
            self.tap_nodes[tap_id] = node_idx
            logger.info(f"ATOMIC TAP NODE STORED: {tap_id} -> {node_idx}")
        
        logger.info(f"ATOMIC NODE ADD SUCCESS: {new_node_count} nodes added, total: {self.num_nodes}")
    
    def _atomic_add_tap_edges(self, valid_connections: List[Dict], new_num_edges: int):
        """Atomically add tap edges to all edge arrays"""
        new_edge_count = len(valid_connections)
        logger.info(f"ATOMIC EDGE ADD: Adding {new_edge_count} edges")
        
        if new_edge_count == 0:
            return
        
        if self.use_gpu:
            # GPU version
            new_from_nodes = cp.zeros(new_edge_count, dtype=cp.int32)
            new_to_nodes = cp.zeros(new_edge_count, dtype=cp.int32)
            new_lengths = cp.zeros(new_edge_count, dtype=cp.float32)
            new_types = cp.full(new_edge_count, 3, dtype=cp.int32)  # tap connection type
            new_capacity = cp.ones(new_edge_count, dtype=cp.int32)
            new_cost = cp.ones(new_edge_count, dtype=cp.float32)
            
            for i, conn in enumerate(valid_connections):
                new_from_nodes[i] = conn['from_node']
                new_to_nodes[i] = conn['to_node']
                new_lengths[i] = conn['length']
                new_cost[i] = conn['cost']
            
            # Concatenate to existing arrays
            self.edge_from_nodes = cp.concatenate([self.edge_from_nodes, new_from_nodes])
            self.edge_to_nodes = cp.concatenate([self.edge_to_nodes, new_to_nodes])
            self.edge_lengths = cp.concatenate([self.edge_lengths, new_lengths])
            self.edge_types = cp.concatenate([self.edge_types, new_types])
            self.edge_capacity = cp.concatenate([self.edge_capacity, new_capacity])
            self.edge_base_cost = cp.concatenate([self.edge_base_cost, new_cost])
        else:
            # CPU version
            new_from_nodes = np.zeros(new_edge_count, dtype=np.int32)
            new_to_nodes = np.zeros(new_edge_count, dtype=np.int32)
            new_lengths = np.zeros(new_edge_count, dtype=np.float32)
            new_types = np.full(new_edge_count, 3, dtype=np.int32)
            new_capacity = np.ones(new_edge_count, dtype=np.int32)
            new_cost = np.ones(new_edge_count, dtype=np.float32)
            
            for i, conn in enumerate(valid_connections):
                new_from_nodes[i] = conn['from_node']
                new_to_nodes[i] = conn['to_node']
                new_lengths[i] = conn['length']
                new_cost[i] = conn['cost']
            
            self.edge_from_nodes = np.concatenate([self.edge_from_nodes, new_from_nodes])
            self.edge_to_nodes = np.concatenate([self.edge_to_nodes, new_to_nodes])
            self.edge_lengths = np.concatenate([self.edge_lengths, new_lengths])
            self.edge_types = np.concatenate([self.edge_types, new_types])
            self.edge_capacity = np.concatenate([self.edge_capacity, new_capacity])
            self.edge_base_cost = np.concatenate([self.edge_base_cost, new_cost])
        
        # Update edge mappings
        for i, conn in enumerate(valid_connections):
            edge_id = f"tap_edge_{i}"
            edge_idx = self.num_edges + i
            self.edge_ids.append(edge_id)
            self.edge_id_to_idx[edge_id] = edge_idx
        
        # CRITICAL: Also populate tap_edge_connections for PathFinder neighbor lookup
        if not hasattr(self, 'tap_edge_connections'):
            self.tap_edge_connections = {}
            
        # Store tap connections separately - PathFinder can query both structures
        for conn in valid_connections:
            from_node = conn['from_node']
            to_node = conn['to_node']
            
            # Add bidirectional connections
            if from_node not in self.tap_edge_connections:
                self.tap_edge_connections[from_node] = []
            if to_node not in self.tap_edge_connections:
                self.tap_edge_connections[to_node] = []
                
            self.tap_edge_connections[from_node].append({
                'to_node': to_node,
                'edge_idx': conn.get('edge_idx', 1)
            })
            self.tap_edge_connections[to_node].append({
                'to_node': from_node, 
                'edge_idx': conn.get('edge_idx', 1)
            })
        
        # Update edge count
        self.num_edges = new_num_edges
        
        logger.info(f"ATOMIC EDGE ADD SUCCESS: {new_edge_count} edges added, total: {self.num_edges}")
        logger.info(f"TAP CONNECTIONS: Stored {len(valid_connections)} bidirectional tap connections")
    
    def _atomic_extend_pathfinder_state(self, new_num_nodes: int):
        """Recreate PathFinder state with current array sizes"""
        logger.info(f"ATOMIC PATHFINDER RECREATE: Building new state for {new_num_nodes} nodes, {self.num_edges} edges")
        
        # PRESERVE tap_nodes before recreation - this is critical!
        tap_nodes_backup = self.tap_nodes.copy()
        
        # CRITICAL FIX: Recreate entire PathFinder state instead of extending
        # This ensures all edge arrays (edge_cost, edge_usage, etc.) match current sizes
        self.pathfinder_state = self._create_pathfinder_state()
        
        # RESTORE tap_nodes after recreation
        self.tap_nodes = tap_nodes_backup
        logger.info(f"PRESERVED {len(self.tap_nodes)} tap nodes through PathFinder recreation")
        
        # Verify state consistency
        state = self.pathfinder_state
        node_array_size = len(state.distance)
        edge_array_size = len(state.edge_cost)
        
        if node_array_size != new_num_nodes:
            raise RuntimeError(f"PathFinder node arrays size {node_array_size} != expected {new_num_nodes}")
            
        if edge_array_size != self.num_edges:
            raise RuntimeError(f"PathFinder edge arrays size {edge_array_size} != expected {self.num_edges}")
        
        logger.info(f"PATHFINDER RECREATE SUCCESS: {node_array_size} nodes, {edge_array_size} edges")
    
    def _validate_node_arrays_consistency(self) -> Dict:
        """Validate node array consistency"""
        issues = []
        
        # Check all node arrays have consistent sizes
        expected_size = self.num_nodes
        arrays = {
            'node_positions': self.node_positions.shape[0] if hasattr(self, 'node_positions') else 0,
            'node_layers': self.node_layers.shape[0] if hasattr(self, 'node_layers') else 0,
            'node_types': self.node_types.shape[0] if hasattr(self, 'node_types') else 0,
            'node_capacity': self.node_capacity.shape[0] if hasattr(self, 'node_capacity') else 0,
            'node_base_cost': self.node_base_cost.shape[0] if hasattr(self, 'node_base_cost') else 0,
        }
        
        for name, size in arrays.items():
            if size != expected_size:
                issues.append(f"{name} size {size} != expected {expected_size}")
        
        # Check node ID mappings
        if len(self.node_ids) != expected_size:
            issues.append(f"node_ids length {len(self.node_ids)} != expected {expected_size}")
        
        if len(self.node_id_to_idx) != expected_size:
            issues.append(f"node_id_to_idx length {len(self.node_id_to_idx)} != expected {expected_size}")
        
        return {'valid': len(issues) == 0, 'issues': issues}
    
    def _validate_pathfinder_arrays_consistency(self) -> Dict:
        """Validate PathFinder state array consistency"""
        issues = []
        
        if not hasattr(self, 'pathfinder_state') or self.pathfinder_state is None:
            issues.append("PathFinder state not initialized")
            return {'valid': False, 'issues': issues}
        
        state = self.pathfinder_state
        expected_size = self.num_nodes
        
        arrays = {
            'distance': len(state.distance),
            'parent_node': len(state.parent_node),
            'parent_edge': len(state.parent_edge),
            'visited': len(state.visited),
            'node_cost': len(state.node_cost),
            'node_usage': len(state.node_usage),
            'node_capacity': len(state.node_capacity),
            'node_pres_cost': len(state.node_pres_cost),
            'node_hist_cost': len(state.node_hist_cost)
        }
        
        for name, size in arrays.items():
            if size != expected_size:
                issues.append(f"PathFinder {name} size {size} != expected {expected_size}")
        
        return {'valid': len(issues) == 0, 'issues': issues}
    
    def remove_temporary_taps(self):
        """Remove temporary tap nodes and restore original RRG size"""
        if not hasattr(self, '_pre_tap_node_count'):
            return
            
        logger.debug("Removing temporary tap nodes")
        
        # Restore original node/edge counts
        self.num_nodes = self._pre_tap_node_count  
        self.num_edges = self._pre_tap_edge_count
        
        # Truncate arrays back to original size
        if self.use_gpu:
            self.node_positions = self.node_positions[:self.num_nodes]
            self.node_layers = self.node_layers[:self.num_nodes]
            self.node_types = self.node_types[:self.num_nodes]
            self.node_capacity = self.node_capacity[:self.num_nodes]
            self.node_base_cost = self.node_base_cost[:self.num_nodes]
            
            self.edge_lengths = self.edge_lengths[:self.num_edges]
            self.edge_types = self.edge_types[:self.num_edges]
            self.edge_capacity = self.edge_capacity[:self.num_edges]
            self.edge_base_cost = self.edge_base_cost[:self.num_edges]
        else:
            # CPU version
            self.node_positions = self.node_positions[:self.num_nodes]
            self.node_layers = self.node_layers[:self.num_nodes]
            self.node_types = self.node_types[:self.num_nodes]
            self.node_capacity = self.node_capacity[:self.num_nodes]
            self.node_base_cost = self.node_base_cost[:self.num_nodes]
            
            self.edge_lengths = self.edge_lengths[:self.num_edges]
            self.edge_types = self.edge_types[:self.num_edges]
            self.edge_capacity = self.edge_capacity[:self.num_edges] 
            self.edge_base_cost = self.edge_base_cost[:self.num_edges]
        
        # Restore PathFinder state to original size
        self._truncate_pathfinder_state()
        
        # Clean up temporary data
        delattr(self, '_pre_tap_node_count')
        delattr(self, '_pre_tap_edge_count')
        
        logger.debug(f"Restored RRG to original size: {self.num_nodes} nodes, {self.num_edges} edges")
    
    def _add_net_tap_nodes(self, net_name: str, net_tap_candidates: Dict[str, List]):
        """Add tap nodes for a specific net"""
        net_taps = net_tap_candidates.get(net_name, [])
        if not net_taps:
            return
            
        # Create new node arrays to append
        new_node_count = len(net_taps)
        logger.debug(f"Adding {new_node_count} tap nodes for {net_name}")
        
        try:
            if self.use_gpu:
                logger.debug(f"Creating new GPU arrays for {new_node_count} tap nodes...")
                new_positions = cp.zeros((new_node_count, 2), dtype=cp.float32)
                new_layers = cp.zeros(new_node_count, dtype=cp.int32)
                new_types = cp.full(new_node_count, 2, dtype=cp.int32)  # switch type
                new_capacity = cp.ones(new_node_count, dtype=cp.int32)
                new_cost = cp.ones(new_node_count, dtype=cp.float32)
                
                for i, tap in enumerate(net_taps):
                    new_positions[i, 0] = tap.tap_x
                    new_positions[i, 1] = tap.tap_y
                    new_layers[i] = tap.via_layers[1]  # Target layer
                
                logger.debug(f"Appending to existing GPU arrays: positions {self.node_positions.shape}, layers {self.node_layers.shape}")
                
                # Ensure all arrays are CuPy arrays before concatenation
                if not isinstance(self.node_positions, cp.ndarray):
                    logger.error(f"node_positions is {type(self.node_positions)}, converting to cupy")
                    self.node_positions = cp.asarray(self.node_positions, dtype=cp.float32)
                if not isinstance(self.node_layers, cp.ndarray):
                    logger.error(f"node_layers is {type(self.node_layers)}, converting to cupy")  
                    self.node_layers = cp.asarray(self.node_layers, dtype=cp.int32)
                if not isinstance(self.node_types, cp.ndarray):
                    logger.error(f"node_types is {type(self.node_types)}, converting to cupy")
                    self.node_types = cp.asarray(self.node_types, dtype=cp.int32)
                if not isinstance(self.node_capacity, cp.ndarray):
                    logger.error(f"node_capacity is {type(self.node_capacity)}, converting to cupy")
                    self.node_capacity = cp.asarray(self.node_capacity, dtype=cp.int32)
                if not isinstance(self.node_base_cost, cp.ndarray):
                    logger.error(f"node_base_cost is {type(self.node_base_cost)}, converting to cupy")
                    self.node_base_cost = cp.asarray(self.node_base_cost, dtype=cp.float32)
                
                # Now safely append to existing arrays - with detailed error handling
                try:
                    self.node_positions = cp.vstack([self.node_positions, new_positions])
                    logger.debug(f"vstack successful")
                except Exception as e:
                    logger.error(f"vstack failed: {e}, types: {type(self.node_positions)}, {type(new_positions)}")
                    raise e
                    
                try:
                    self.node_layers = cp.concatenate([self.node_layers, new_layers])
                    logger.debug(f"node_layers concatenate successful")
                except Exception as e:
                    logger.error(f"node_layers concatenate failed: {e}, types: {type(self.node_layers)}, {type(new_layers)}")
                    raise e
                    
                try:
                    self.node_types = cp.concatenate([self.node_types, new_types])
                    logger.debug(f"node_types concatenate successful")
                except Exception as e:
                    logger.error(f"node_types concatenate failed: {e}, types: {type(self.node_types)}, {type(new_types)}")
                    raise e
                    
                try:
                    self.node_capacity = cp.concatenate([self.node_capacity, new_capacity])
                    logger.debug(f"node_capacity concatenate successful")
                except Exception as e:
                    logger.error(f"node_capacity concatenate failed: {e}, types: {type(self.node_capacity)}, {type(new_capacity)}")
                    raise e
                    
                try:
                    self.node_base_cost = cp.concatenate([self.node_base_cost, new_cost])
                    logger.debug(f"node_base_cost concatenate successful")
                except Exception as e:
                    logger.error(f"node_base_cost concatenate failed: {e}, types: {type(self.node_base_cost)}, {type(new_cost)}")
                    raise e
                    
                logger.debug(f"Successfully appended to GPU arrays")
            else:
                # CPU version
                new_positions = np.zeros((new_node_count, 2), dtype=np.float32)
                new_layers = np.zeros(new_node_count, dtype=np.int32)
                new_types = np.full(new_node_count, 2, dtype=np.int32)
                new_capacity = np.ones(new_node_count, dtype=np.int32)
                new_cost = np.ones(new_node_count, dtype=np.float32)
                
                for i, tap in enumerate(net_taps):
                    new_positions[i, 0] = tap.tap_x
                    new_positions[i, 1] = tap.tap_y
                    new_layers[i] = tap.via_layers[1]
                    
                self.node_positions = np.vstack([self.node_positions, new_positions])
                self.node_layers = np.concatenate([self.node_layers, new_layers])
                self.node_types = np.concatenate([self.node_types, new_types])
                self.node_capacity = np.concatenate([self.node_capacity, new_capacity])
                self.node_base_cost = np.concatenate([self.node_base_cost, new_cost])
            
            # Update node IDs and mapping (outside the if/else but inside try)
            logger.debug(f"Updating node IDs and mapping...")
            start_idx = self.num_nodes  # Store original count before updating
            self.num_nodes += new_node_count  # Update count first
            
            for i, tap in enumerate(net_taps):
                tap_id = f"tap_{net_name}_{i}"
                node_idx = start_idx + i  # Use stored original count
                self.node_ids.append(tap_id)
                self.node_id_to_idx[tap_id] = node_idx
            logger.debug(f"Successfully added {new_node_count} tap nodes, total nodes now: {self.num_nodes}")
            
        except Exception as e:
            logger.error(f"Error adding tap nodes for {net_name}: {e}")
            logger.error(f"Debug info: node_positions type={type(self.node_positions)}, node_layers type={type(self.node_layers)}")
            logger.error(f"Debug info: node_types type={type(self.node_types)}, node_capacity type={type(self.node_capacity)}")
            logger.error(f"Debug info: node_base_cost type={type(self.node_base_cost)}")
            logger.error(f"Debug info: new_positions type={type(new_positions) if 'new_positions' in locals() else 'undefined'}")
            logger.error(f"Debug info: new_layers type={type(new_layers) if 'new_layers' in locals() else 'undefined'}")
            raise e
    
    def _add_net_tap_edges(self, net_name: str, net_tap_candidates: Dict[str, List]):
        """Add tap edges connecting tap nodes to Manhattan fabric"""
        
        net_taps = net_tap_candidates.get(net_name, [])
        if not net_taps:
            return
        
        logger.info(f"Adding tap edges for {len(net_taps)} tap nodes in {net_name}")
        
        # REAL PATHFINDER: Create proper grid-based tap connections using spatial search
        # This is required for proper PathFinder routing on the orthogonal grid
        new_edges = []
        edge_start_idx = self.num_edges
        
        logger.info(f"REAL PATHFINDER: Creating proper grid-based tap connections for {len(net_taps)} taps")
        
        # GRID-BASED CONNECTION: Use spatial search to connect taps to actual grid nodes
        logger.info(f"GRID ROUTING: Finding nearest grid nodes for {len(net_taps)} taps")
        
        for i, tap in enumerate(net_taps):
            # Use the proper tap node ID that was created and mapped
            tap_id = f"tap_{net_name}_{i}"
            tap_node_idx = self.node_id_to_idx[tap_id]  # Get actual unique index
            
            # GRID-BASED SEARCH: Find actual grid nodes near tap position
            # This ensures proper PathFinder routing on the orthogonal grid
            tap_pos = net_taps[i]
            tap_x, tap_y = tap_pos.tap_x, tap_pos.tap_y
            connected_count = 0
            
            # Use spatial index to find nearby grid nodes efficiently
            if hasattr(self, 'spatial_index') and self.spatial_index:
                # Snap tap position to grid
                grid_x = round(tap_x / self.spatial_grid_size)
                grid_y = round(tap_y / self.spatial_grid_size)
                
                # Search nearby grid cells for connecting nodes
                for dx in [-1, 0, 1]:  # Search 3x3 grid area
                    for dy in [-1, 0, 1]:
                        for layer in range(2):  # Check both horizontal and vertical layers
                            key = (grid_x + dx, grid_y + dy, layer)
                            if key in self.spatial_index:
                                # Connect to all nodes in this grid cell
                                for fabric_node_idx in self.spatial_index[key][:2]:  # Limit to 2 connections
                                    # Create bidirectional connection to grid node
                                    distance = 0.2  # Grid spacing
                                    new_edges.append({
                                        'from_node': tap_node_idx,
                                        'to_node': fabric_node_idx,
                                        'length': distance,
                                        'type': 3,  # tap_edge type
                                        'capacity': 1,
                                        'cost': 1.0
                                    })
                                    new_edges.append({
                                        'from_node': fabric_node_idx,
                                        'to_node': tap_node_idx,
                                        'length': distance,
                                        'type': 3,  # tap_edge type
                                        'capacity': 1,
                                        'cost': 1.0
                                    })
                                    connected_count += 1
            
            if connected_count == 0:
                logger.error(f"GRID ROUTING: Failed to connect tap {i} to grid nodes")
            else:
                logger.debug(f"GRID ROUTING: Connected tap {i} to {connected_count} grid nodes")
        
        if new_edges:
            # Add edges to the RRG data structures
            if self.use_gpu:
                # GPU version - extend edge arrays
                new_edge_count = len(new_edges)
                
                new_from_nodes = cp.zeros(new_edge_count, dtype=cp.int32)
                new_to_nodes = cp.zeros(new_edge_count, dtype=cp.int32)
                new_lengths = cp.zeros(new_edge_count, dtype=cp.float32)
                new_types = cp.full(new_edge_count, 3, dtype=cp.int32)  # tap_edge
                new_capacity = cp.ones(new_edge_count, dtype=cp.int32)
                new_cost = cp.ones(new_edge_count, dtype=cp.float32)
                
                for idx, edge in enumerate(new_edges):
                    new_from_nodes[idx] = edge['from_node']
                    new_to_nodes[idx] = edge['to_node']
                    new_lengths[idx] = edge['length']
                    new_cost[idx] = edge['cost']
                
                # Extend existing arrays
                self.edge_from_nodes = cp.concatenate([self.edge_from_nodes, new_from_nodes])
                self.edge_to_nodes = cp.concatenate([self.edge_to_nodes, new_to_nodes])
                self.edge_lengths = cp.concatenate([self.edge_lengths, new_lengths])
                self.edge_types = cp.concatenate([self.edge_types, new_types])
                self.edge_capacity = cp.concatenate([self.edge_capacity, new_capacity])
                self.edge_base_cost = cp.concatenate([self.edge_base_cost, new_cost])
                
            else:
                # CPU version - extend edge arrays
                new_edge_count = len(new_edges)
                
                new_from_nodes = np.zeros(new_edge_count, dtype=np.int32)
                new_to_nodes = np.zeros(new_edge_count, dtype=np.int32)
                new_lengths = np.zeros(new_edge_count, dtype=np.float32)
                new_types = np.full(new_edge_count, 3, dtype=np.int32)
                new_capacity = np.ones(new_edge_count, dtype=np.int32)
                new_cost = np.ones(new_edge_count, dtype=np.float32)
                
                for idx, edge in enumerate(new_edges):
                    new_from_nodes[idx] = edge['from_node']
                    new_to_nodes[idx] = edge['to_node']
                    new_lengths[idx] = edge['length']
                    new_cost[idx] = edge['cost']
                
                # Extend existing arrays
                self.edge_from_nodes = np.concatenate([self.edge_from_nodes, new_from_nodes])
                self.edge_to_nodes = np.concatenate([self.edge_to_nodes, new_to_nodes])
                self.edge_lengths = np.concatenate([self.edge_lengths, new_lengths])
                self.edge_types = np.concatenate([self.edge_types, new_types])
                self.edge_capacity = np.concatenate([self.edge_capacity, new_capacity])
                self.edge_base_cost = np.concatenate([self.edge_base_cost, new_cost])
            
            # Update edge count and IDs
            old_edge_count = self.num_edges
            self.num_edges += len(new_edges)
            
            # Add edge IDs
            for idx, edge in enumerate(new_edges):
                edge_id = f"tap_edge_{net_name}_{idx}"
                self.edge_ids.append(edge_id)
            
            # Update adjacency matrix with new edges for PathFinder
            self._update_adjacency_matrix_with_edges(new_edges)
            
            logger.info(f"Added {len(new_edges)} tap edges for {net_name} (total edges: {self.num_edges})")
        else:
            logger.warning(f"WARNING: No tap edges created for {net_name} - tap nodes may be isolated")
    
    def _update_adjacency_matrix_with_edges(self, new_edges):
        """Update adjacency matrix with new edges - OPTIMIZED VERSION"""
        if not new_edges:
            return
            
        logger.info(f"OPTIMIZED: Adding {len(new_edges)} new tap edges without rebuilding adjacency matrix")
        
        # PERFORMANCE FIX: Don't rebuild the 26M+ edge adjacency matrix for just 80 tap edges
        # Instead, maintain tap edges separately for PathFinder queries
        
        if not hasattr(self, 'tap_edge_connections'):
            self.tap_edge_connections = {}
            
        # Store tap connections separately - PathFinder can query both structures
        for edge in new_edges:
            from_node = edge['from_node']
            to_node = edge['to_node']
            
            # Add bidirectional connections
            if from_node not in self.tap_edge_connections:
                self.tap_edge_connections[from_node] = []
            if to_node not in self.tap_edge_connections:
                self.tap_edge_connections[to_node] = []
                
            self.tap_edge_connections[from_node].append({
                'target': to_node,
                'edge_idx': edge.get('edge_idx', 1)
            })
            self.tap_edge_connections[to_node].append({
                'target': from_node, 
                'edge_idx': edge.get('edge_idx', 1)
            })
        
        logger.info(f"PERFORMANCE: Tap edges stored separately - avoiding 26M+ edge matrix rebuild")
        logger.info(f"ROUTING READY: Main adjacency matrix preserved, tap connections available")
    
    def _extend_pathfinder_state(self):
        """Extend PathFinder state arrays to match new node count"""
        if not hasattr(self, 'pathfinder_state'):
            logger.warning("No pathfinder_state to extend")
            return
            
        state = self.pathfinder_state
        old_size = len(state.distance)
        new_size = self.num_nodes
        
        logger.info(f"PathFinder state extension: old_size={old_size}, new_size={new_size}, self.num_nodes={self.num_nodes}")
        
        if new_size <= old_size:
            logger.info(f"No extension needed: new_size ({new_size}) <= old_size ({old_size})")
            return
            
        additional_nodes = new_size - old_size
        logger.info(f"Extending PathFinder state arrays by {additional_nodes} nodes")
        
        if self.use_gpu:
            # Extend GPU arrays with detailed error handling
            try:
                state.distance = cp.concatenate([state.distance, cp.full(additional_nodes, cp.inf)])
            except Exception as e:
                logger.error(f"Error extending state.distance: {e}, type: {type(state.distance)}")
                raise e
                
            try:
                state.parent_node = cp.concatenate([state.parent_node, cp.full(additional_nodes, -1)])
            except Exception as e:
                logger.error(f"Error extending state.parent_node: {e}, type: {type(state.parent_node)}")
                raise e
                
            try:
                state.parent_edge = cp.concatenate([state.parent_edge, cp.full(additional_nodes, -1)])
            except Exception as e:
                logger.error(f"Error extending state.parent_edge: {e}, type: {type(state.parent_edge)}")
                raise e
                
            try:
                state.visited = cp.concatenate([state.visited, cp.zeros(additional_nodes, dtype=cp.bool_)])
            except Exception as e:
                logger.error(f"Error extending state.visited: {e}, type: {type(state.visited)}")
                raise e
            
            # Extend other PathFinder arrays with error handling
            try:
                state.node_cost = cp.concatenate([state.node_cost, cp.ones(additional_nodes)])
            except Exception as e:
                logger.error(f"Error extending state.node_cost: {e}, type: {type(state.node_cost)}")
                raise e
                
            try:
                state.node_usage = cp.concatenate([state.node_usage, cp.zeros(additional_nodes, dtype=cp.int32)])
            except Exception as e:
                logger.error(f"Error extending state.node_usage: {e}, type: {type(state.node_usage)}")
                raise e
                
            try:
                state.node_capacity = cp.concatenate([state.node_capacity, cp.ones(additional_nodes, dtype=cp.int32)])
            except Exception as e:
                logger.error(f"Error extending state.node_capacity: {e}, type: {type(state.node_capacity)}")
                raise e
                
            try:
                state.node_pres_cost = cp.concatenate([state.node_pres_cost, cp.zeros(additional_nodes)])
            except Exception as e:
                logger.error(f"Error extending state.node_pres_cost: {e}, type: {type(state.node_pres_cost)}")
                raise e
                
            try:
                state.node_hist_cost = cp.concatenate([state.node_hist_cost, cp.zeros(additional_nodes)])
            except Exception as e:
                logger.error(f"Error extending state.node_hist_cost: {e}, type: {type(state.node_hist_cost)}")
                raise e
        else:
            # CPU version
            state.distance = np.concatenate([state.distance, np.full(additional_nodes, np.inf)])
            state.parent_node = np.concatenate([state.parent_node, np.full(additional_nodes, -1)])
            state.parent_edge = np.concatenate([state.parent_edge, np.full(additional_nodes, -1)])
            state.visited = np.concatenate([state.visited, np.zeros(additional_nodes, dtype=np.bool_)])
            
            state.node_cost = np.concatenate([state.node_cost, np.ones(additional_nodes)])
            state.node_usage = np.concatenate([state.node_usage, np.zeros(additional_nodes, dtype=np.int32)])
            state.node_capacity = np.concatenate([state.node_capacity, np.ones(additional_nodes, dtype=np.int32)])
            state.node_pres_cost = np.concatenate([state.node_pres_cost, np.zeros(additional_nodes)])
            state.node_hist_cost = np.concatenate([state.node_hist_cost, np.zeros(additional_nodes)])
        
        # Verify successful extension
        final_size = len(state.distance)
        logger.info(f"PathFinder state extension completed: final_size={final_size}, expected={new_size}")
        if final_size != new_size:
            logger.error(f"PathFinder state extension FAILED: got size {final_size}, expected {new_size}")
        else:
            logger.info(f"PathFinder state extension SUCCESS: arrays now size {final_size}")
    
    def _truncate_pathfinder_state(self):
        """Truncate PathFinder state arrays back to original size"""
        if not hasattr(self, 'pathfinder_state'):
            return
            
        # PERFORMANCE FIX: Skip PathFinder state truncation to prevent hang on 9M+ nodes
        logger.info("PERFORMANCE MODE: Skipping PathFinder state truncation to prevent hang")
        logger.info("PathFinder state arrays will remain at extended size for performance")
        # The truncation of large GPU arrays is very expensive and causes hangs
        # For production routing, we can skip this cleanup step
        return
        
        # Original truncation code (disabled for performance)
        state = self.pathfinder_state
        target_size = self.num_nodes
        
        # Truncate all PathFinder arrays
        state.distance = state.distance[:target_size]
        state.parent_node = state.parent_node[:target_size]
        state.parent_edge = state.parent_edge[:target_size]
        state.visited = state.visited[:target_size]
        state.node_cost = state.node_cost[:target_size]
        state.node_usage = state.node_usage[:target_size]
        state.node_capacity = state.node_capacity[:target_size]
        state.node_pres_cost = state.node_pres_cost[:target_size]
        state.node_hist_cost = state.node_hist_cost[:target_size]
    
    def _build_spatial_index(self):
        """Build optimized spatial index for O(1) fabric node lookup by (x,y,layer)"""
        
        logger.info("Building fast spatial index for tap-to-fabric connections...")
        
        # ULTRA-FAST SPATIAL INDEX: Skip for now since tap connections use separate storage
        # The tap-to-fabric connections are now stored in tap_edge_connections dict
        # which is much faster than building a spatial index over 9M+ nodes
        
        logger.info("PERFORMANCE OPTIMIZATION: Using direct tap-to-fabric connections instead of spatial index")
        logger.info("Spatial index building skipped - using tap_edge_connections for connectivity")
        
        # Create minimal spatial index for debugging/fallback only
        self.spatial_index = {}
        self.spatial_grid_size = 0.2  # mm - Match the routing grid size
        
        logger.info(f"Fast spatial index bypassed - using direct tap connections for performance")
    
    def validate_indexing_consistency(self) -> Dict[str, bool]:
        """Comprehensive validation of indexing consistency across all GPU RRG components"""
        
        results = {
            'node_count_consistent': True,
            'pathfinder_arrays_consistent': True,
            'adjacency_matrix_valid': True,
            'tap_nodes_valid': True,
            'overall_valid': True
        }
        
        issues = []
        
        try:
            # 1. Node count consistency
            expected_nodes = len(self.node_ids)
            if self.num_nodes != expected_nodes:
                issues.append(f"Node count mismatch: self.num_nodes={self.num_nodes}, len(node_ids)={expected_nodes}")
                results['node_count_consistent'] = False
            
            # 2. PathFinder state arrays consistency
            if hasattr(self, 'pathfinder_state') and self.pathfinder_state:
                state = self.pathfinder_state
                state_arrays = {
                    'distance': len(state.distance),
                    'parent_node': len(state.parent_node),
                    'parent_edge': len(state.parent_edge),
                    'visited': len(state.visited),
                    'node_cost': len(state.node_cost),
                    'node_usage': len(state.node_usage),
                    'node_capacity': len(state.node_capacity),
                    'node_pres_cost': len(state.node_pres_cost),
                    'node_hist_cost': len(state.node_hist_cost)
                }
                
                for name, size in state_arrays.items():
                    if size != self.num_nodes:
                        issues.append(f"PathFinder array '{name}' size {size} != num_nodes {self.num_nodes}")
                        results['pathfinder_arrays_consistent'] = False
            
            # 3. Adjacency matrix validation
            if hasattr(self, 'adjacency_matrix') and self.adjacency_matrix is not None:
                matrix_shape = self.adjacency_matrix.shape
                expected_shape = (self.num_nodes, self.num_nodes)
                
                if matrix_shape != expected_shape:
                    issues.append(f"Adjacency matrix shape {matrix_shape} != expected {expected_shape}")
                    results['adjacency_matrix_valid'] = False
                
                indptr_size = len(self.adjacency_matrix.indptr)
                expected_indptr_size = self.num_nodes + 1
                
                if indptr_size != expected_indptr_size:
                    issues.append(f"Adjacency matrix indptr size {indptr_size} != expected {expected_indptr_size}")
                    results['adjacency_matrix_valid'] = False
            
            # 4. Tap node validation
            if hasattr(self, 'node_id_to_idx') and self.node_id_to_idx:
                max_node_idx = max(self.node_id_to_idx.values()) if self.node_id_to_idx else -1
                
                if max_node_idx >= self.num_nodes:
                    issues.append(f"Maximum node index {max_node_idx} >= num_nodes {self.num_nodes}")
                    results['tap_nodes_valid'] = False
                
                # Check tap node indices specifically
                tap_nodes = {k: v for k, v in self.node_id_to_idx.items() if k.startswith('tap_')}
                for tap_id, tap_idx in tap_nodes.items():
                    if tap_idx < 0 or tap_idx >= self.num_nodes:
                        issues.append(f"Tap node '{tap_id}' has invalid index {tap_idx} (valid range: [0, {self.num_nodes-1}])")
                        results['tap_nodes_valid'] = False
            
            # Overall validation
            results['overall_valid'] = all(results.values())
            
            if issues:
                logger.error(f"GPU RRG Indexing Validation FAILED with {len(issues)} issues:")
                for issue in issues[:10]:  # Show first 10 issues
                    logger.error(f"  - {issue}")
                if len(issues) > 10:
                    logger.error(f"  ... and {len(issues) - 10} more issues")
            else:
                logger.info("GPU RRG Indexing Validation PASSED - all components consistent")
            
        except Exception as e:
            logger.error(f"GPU RRG Indexing Validation FAILED with exception: {e}")
            results['overall_valid'] = False
            issues.append(f"Validation exception: {e}")
        
        return results
    
    def _find_nearby_fabric_nodes(self, tap_x: float, tap_y: float, tap_layer: int, radius_mm: float = 1.0):
        """Fast spatial lookup for fabric nodes near tap position"""
        
        nearby_nodes = []
        
        # Convert radius to grid cells
        radius_cells = int(radius_mm / self.spatial_grid_size) + 1
        
        # Snap tap position to grid
        center_grid_x = round(tap_x / self.spatial_grid_size)
        center_grid_y = round(tap_y / self.spatial_grid_size)
        
        # Search in expanding square around tap position
        for dx in range(-radius_cells, radius_cells + 1):
            for dy in range(-radius_cells, radius_cells + 1):
                grid_x = center_grid_x + dx
                grid_y = center_grid_y + dy
                
                key = (grid_x, grid_y, tap_layer)
                
                if key in self.spatial_index:
                    for node_idx in self.spatial_index[key]:
                        # Calculate actual distance
                        if self.use_gpu:
                            node_x, node_y = float(self.node_positions[node_idx, 0]), float(self.node_positions[node_idx, 1])
                        else:
                            node_x, node_y = float(self.node_positions[node_idx, 0]), float(self.node_positions[node_idx, 1])
                        
                        distance = ((node_x - tap_x)**2 + (node_y - tap_y)**2)**0.5
                        
                        if distance <= radius_mm:
                            nearby_nodes.append((node_idx, node_x, node_y, distance))
        
        # Sort by distance
        nearby_nodes.sort(key=lambda x: x[3])
        return nearby_nodes